<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="选择有时候比努力更重要">
<meta property="og:type" content="website">
<meta property="og:title" content="Quentin 的世界">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Quentin 的世界">
<meta property="og:description" content="选择有时候比努力更重要">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Quentin">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Quentin 的世界</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Quentin 的世界</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(4%E6%9D%A1%E6%B6%88%E6%81%AF)Python%E9%81%97%E4%BC%A0%E5%92%8C%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%88%E4%BA%8C%EF%BC%89Geatpy%E5%BA%93%E5%87%BD%E6%95%B0%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_jazzbin%E7%9A%84%E5%8D%9A%E5%AE%A2-CSDN%E5%8D%9A%E5%AE%A2_geatpy2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(4%E6%9D%A1%E6%B6%88%E6%81%AF)Python%E9%81%97%E4%BC%A0%E5%92%8C%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%88%E4%BA%8C%EF%BC%89Geatpy%E5%BA%93%E5%87%BD%E6%95%B0%E5%92%8C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84_jazzbin%E7%9A%84%E5%8D%9A%E5%AE%A2-CSDN%E5%8D%9A%E5%AE%A2_geatpy2/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:49" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:49+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-17 04:45:04" itemprop="dateModified" datetime="2022-12-17T04:45:04+08:00">2022-12-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>上一篇讲了Geatpy的快速入门：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_33353186/article/details/82014986">https://blog.csdn.net/qq_33353186/article/details/82014986</a></p>
<p>但是光是几个例子是远远不能熟练掌握python遗传和进化算法编程的，得进一步了解其原理以及API。</p>
<p>Geatpy提供面向对象的简单封装的开放式进化算法框架，可以方便、自由地与其他算法或项目相结合。Geatpy的文件结构如下图所示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F05BE0B3F1DF34B1EB1980E741AE1AD63.png"></p>
<p>其中“core”文件夹里面全部为Geatpy工具箱的内核函数；“templates”文件夹存放的是Geatpy的进化算法模板；“testbed”是进化算法的测试平台，内含多种单目标优化、多目标优化、组合优化的测试集。“demo”文件夹中包含了应用Geatpy工具箱求解问题的案例。“operators”是2.2.2版本之后新增的，里面存放着面向对象的重组和变异算子类，这些重组和变异算子类通过调用“core”文件夹下的重组和变异算子来执行相关的操作。</p>
<p>Geatpy 的面向对象进化算法框架有四个大类：Algorithm(算法模板顶级父类)、Population(种群类)、PsyPopulation(多染色体种群类) 和Problem(问题类)，分别存放在“Algorithm.py”、“Population.py”、“Problem.py”文件中。其UML 类图如下所示：</p>
<p>Geatpy 的面向对象进化算法框架有四个大类：Algorithm(算法模板顶级父类)、Population(种群类)、PsyPopulation(多染色体种群类) 和Problem(问题类)，分别存放在“Algorithm.py”、“Population.py”、“Problem.py”文件中。其UML 类图如下所示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F533286B98DCF4CC8930FC06FA6464CA4.png"></p>
<p>Problem类定义了与问题相关的一些信息，如问题名称name、优化目标的维数M、决策变量的个数Dim、决策变量的范围ranges、决策变量的边界borders等。maxormins是一个记录着各个目标函数是最小化抑或是最大化的list类型列表，其中元素为1表示对应的目标是最小化目标；为-1表示对应的是最大化目标。例如M&#x3D;3，maxormins&#x3D;[1,-1,1]，此时表示有三个优化目标，其中第一、第三个是最小化目标，第二个是最大化目标。varTypes是一个记录着决策变量类型的行向量，其中的元素为0表示对应的决策变量是连续型变量；为1表示对应的是离散型变量。待求解的目标函数定义在aimFunc()的函数中。calReferObjV()函数则用于计算或读取目标函数参考值（一般用理论上的目标函数的最优值作为参考值），该参考值可以用于后续的指标分析。在实际使用时，不是直接在Problem类的文件中修改相关代码来使用的，而是通过定义一个继承Problem的子类来完成对问题的定义的。这些在后面的章节中会详细讲述。getReferObjV()是Problem父类中已经实现了的一个函数，它先尝试读取特定文件夹中的目标函数值参考数据，如果读取不到，则调用calReferObjV()进行计算。对于Problem类中各属性的详细含义可查看Problem.py源码。</p>
<p>Population类是一个表示种群的类。一个种群包含很多个个体，而每个个体都有一条染色体(若要用多染色体，则使用PsyPopulation类)。除了染色体外，每个个体都有一个译码矩阵Field(或俗称区域描述器)来标识染色体应该如何解码得到表现型，同时也有其对应的目标函数值以及适应度。种群类就是一个把所有个体的这些数据统一存储起来的一个类。比如里面的Chrom是一个存储种群所有个体染色体的矩阵，它的每一行对应一个个体的染色体；ObjV是一个目标函数值矩阵，每一行对应一个个体的所有目标函数值，每一列对应一个目标。对于Population类中各属性的详细含义可查看Population.py源码以及下一章“Geatpy数据结构”。</p>
<p>PsyPopulation类是继承了Population的支持多染色体混合编码的种群类。一个种群包含很多个个体，而每个个体都有多条染色体。用Chroms列表存储所有的染色体矩阵(Chrom)；Encodings列表存储各染色体对应的编码方式(Encoding)；Fields列表存储各染色体对应的译码矩阵(Field)。</p>
<p>Algorithm类是进化算法的核心类。它既存储着跟进化算法相关的一些参数，同时也在其继承类中实现具体的进化算法。比如Geatpy中的moea_NSGA3_templet.py是实现了多目标优化NSGA-III算法的进化算法模板类，它是继承了Algorithm类的具体算法的模板类。关于Algorithm类中各属性的含义可以查看Algorithm.py源码。这些算法模板通过调用Geatpy工具箱提供的进化算法库函数实现对种群的进化操作，同时记录进化过程中的相关信息，其基本层次结构如下图：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F93AABC833935413BBF11CA390A036EB3.png"></p>
<p>Geatpy工具箱提供了大量的跟进化算法有关的内核函数，涵盖了单目标优化、多目标优化、组合优化等方面的众多算子，同时提供指标评价、绘图等功能性函数。下图展示了Geatpy的函数调用关系。中心是进化算法模板，它调用高级的运算函数(selecting, recombin, mutate)来实现进化优化。高级函数进一步调用相关的低级运算函数(即实现选择、交叉、变异等底层算法的函数)。这种层级调用关系使Geatpy的结构十分清晰，更重要的是，你可以自定义更多低级运算函数来轻松自由地扩展Geatpy。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F077FC844A85842529ECF3764632F4B08.png"></p>
<p>这些库函数具体有：</p>
<p>1. 与初始化种群相关的函数</p>
<p>• crtfld (生成译码矩阵，俗称“区域描述器”)</p>
<p>• crtbp (创建二进制种群染色体矩阵)</p>
<p>• crtip (创建元素是整数的种群染色体矩阵)</p>
<p>• crtpp (创建排列编码种群染色体矩阵)</p>
<p>• crtrp (创建元素是实数的种群染色体矩阵)</p>
<p>• meshrng (网格化决策变量范围)</p>
<ol start="2">
<li>进化迭代相关函数</li>
</ol>
<p>当完成了种群染色体的初始化后，就可以进行进化迭代了。这部分是在进化算法模</p>
<p>板里调用。迭代过程中包括：</p>
<p>• 调用ranking 或scaling 等计算种群适应度。</p>
<p>• 调用selecting 进行选择操作(也可以直接调用低级选择函数)。</p>
<p>• 调用recombin 进行重组操作(也可以直接调用低级重组函数)。</p>
<p>• 调用mutate 进行重组操作(也可以直接调用低级变异函数)。</p>
<ol start="3">
<li>适应度计算</li>
</ol>
<p>• ranking (基于等级划分的适应度分配计算)</p>
<p>• scaling (线性尺度变换适应度计算)</p>
<p>• indexing (指数尺度变换适应度计算)</p>
<p>• powing (幂尺度变换适应度计算)</p>
<p>对于多目标进化优化，由于各种多目标优化算法所采用的适应度计算方法门类有很多，因此此时的适应度计算交由继承了Algorithm 的具体算法模板类中实现，详见相关源码。</p>
<ol start="4">
<li>选择</li>
</ol>
<p>selecting 是高级选择函数，它调用下面的低级选择函数：</p>
<p>• dup (Duplication，基于适应度排序的直接复制选择)</p>
<p>• ecs (Elite Copy Selection，精英复制选择)</p>
<p>• etour (精英保留锦标赛选择)</p>
<p>• otos (One-to-One Survivor Selection，一对一生存者选择)</p>
<p>• rcs (Random Compensation Selection，随机补偿选择)</p>
<p>• rps (Random Permutation Selection，随机排列选择)</p>
<p>• rws (Roulette Wheel Selection，轮盘赌选择)</p>
<p>• sus (Stochastic Universal Sampling，随机抽样选择)</p>
<p>• tour (Tournament，锦标赛选择)</p>
<p>• urs (Uncommitted Random Selection，无约束随机选择)</p>
<ol start="5">
<li>重组(包括交叉)</li>
</ol>
<p>交叉是重组的一部分。</p>
<p>• recdis (离散重组)</p>
<p>• recint (中间重组)</p>
<p>• reclin (线性重组)</p>
<p>• recndx (正态分布交叉)</p>
<p>• recsbx (模拟二进制交叉)</p>
<p>• xovbd (二项式分布交叉)</p>
<p>• xovdp (两点交叉)</p>
<p>• xovexp (指数交叉)</p>
<p>• xovmp (多点交叉)</p>
<p>• xovox (顺序交叉)</p>
<p>• xovpmx (部分匹配交叉)</p>
<p>• xovsec (洗牌指数交叉)</p>
<p>• xovsh (洗牌交叉)</p>
<p>• xovsp (单点交叉)</p>
<p>• xovud (均匀分布交叉)</p>
<p>注意：所有重组算子都不会检查重组结果是否满足所设边界范围。下面的变异算子则是内置对边界范围的检查和修复。因此如果在进化算法中要单是使用重组算子，则需要调用“ea.boundfix”函数进行边界修复。详见“help(ea.boundfix)”。</p>
<p>在重组过程中，种群的前一半个体会与后一半个体的染色体按照个体顺序进行一一配对。这些重组算子可通过设置传入参数“Half”的值为True，来使得重组后只保留一半的个体，此时将保留上面所说的一一配对重组后的第一条染色体。</p>
<p>Geatpy2.2.2版本之后在进化算法框架中新增了面向对象的Recombination(重组算子接口)，上述的低级重组算子均有与之对应的重组算子类来进行参数的设置及调用，这些新增的类命名为首字母大写的对应低级重组算子的名称：</p>
<p>• Recdis (离散重组算子类)</p>
<p>• Recint (中间重组算子类)</p>
<p>• Reclin (线性重组算子类)</p>
<p>• Recndx (正态分布交叉算子类)</p>
<p>• Recsbx (模拟二进制交叉算子类)</p>
<p>• Xovbd (二项式分布交叉算子类)</p>
<p>• Xovdp (两点交叉算子类)</p>
<p>• Xovexp (指数交叉算子类)</p>
<p>• Xovmp (多点交叉算子类)</p>
<p>• Xovox (顺序交叉算子类)</p>
<p>• Xovpmx (部分匹配交叉算子类)</p>
<p>• Xovsec (洗牌指数交叉算子类)</p>
<p>• Xovsh (洗牌交叉算子类)</p>
<p>• Xovsp (单点交叉算子类)</p>
<p>• Xovud (均匀分布交叉算子类)</p>
<p>在调用某个重组算子时，可以直接调用低级重组算子进行重组；也可以利用高级重组算子recombine 通过指定低级重组算子的名称来调用低级重组算子进行重组，如recombine(’xovdp’, …)；也可以通过实例化重组算子类的对象，然后执行该对象的do()函数进行重组，例如：recOper &#x3D; Xovdp(…)，recOper.do(…)。具体结构详见这些类的源码。</p>
<ol start="6">
<li>突变</li>
</ol>
<p>mutate 是高级的突变函数，它调用下面的低级突变函数：</p>
<p>• mutbga (Mutation for Breeder Genetic Algorithm，Breeder GA 算法突变算子)</p>
<p>• mutbin (Mutation for Binary Chromosomes，二进制变异算子)</p>
<p>• mutde (Mutation for Differential Evolution，差分变异算子)</p>
<p>• mutgau (Gaussian Mutation，高斯突变算子)</p>
<p>• mutinv (Invertion Mutation，染色体片段逆转变异算子)</p>
<p>• mutmove (Mutation by Moving，染色体片段移位变异算子)</p>
<p>• mutpolyn (Polynomial Mutation，多项式变异)</p>
<p>• mutpp (Mutation of Permutation Chromosomes，排列编码变异算子)</p>
<p>• mutswap (Two Point Swapping Mutation，染色体两点互换变异算子)</p>
<p>• mutuni (Uniform Mutation，均匀变异算子)</p>
<p>注意：对于mutbga、mutde、mutgau、mutpolyn、mutuni，变异是先按实数值来变异，然后对于标记了是离散型变量进行四舍五入。因此结果往往会是浮点“float”类型的，此时如果要把这些离散值用作其他变量的下标，需要对其进行强制类型转换。</p>
<p>Geatpy2.2.2 版本之后在进化算法框架中新增了面向对象的Mutation(变异算子算子</p>
<p>接口)，上述的低级变异算子均有与之对应的变异算子类来进行参数的设置及调用，这</p>
<p>些新增的类命名为首字母大写的对应低级变异算子的名称：</p>
<p>• Mutbga (Mutation for Breeder Genetic Algorithm，Breeder GA 算法突变算子类)</p>
<p>• Mutbin (Mutation for Binary Chromosomes，二进制变异算子类)</p>
<p>• Mutde (Mutation for Differential Evolution，差分变异算子类)</p>
<p>• Mutgau (Gaussian Mutation，高斯突变算子类)</p>
<p>• Mutinv (Invertion Mutation，染色体片段逆转变异算子类)</p>
<p>• Mutmove (Mutation by Moving，染色体片段移位变异算子类)</p>
<p>• Mutpolyn (Polynomial Mutation，多项式变异类)</p>
<p>• Mutpp (Mutation of Permutation Chromosomes，排列编码变异算子类)</p>
<p>• Mutswap (Two Point Swapping Mutation，染色体两点互换变异算子类)</p>
<p>• Mutuni (Uniform Mutation，均匀变异算子类)</p>
<p>在调用某个变异算子时，可以直接调用低级变异算子进行重组；也可以利用高级变异算子mutate 通过指定低级变异算子的名称来调用低级变异算子进行重组，如mutate(’mutgau’, …)；也可以通过实例化变异算子类的对象，然后执行该对象的do() 函数进行变异，例如：mutOper &#x3D; Mutgau(…)，mutOper.do(…)。具体结构详见这些类的源码。</p>
<ol start="7">
<li>染色体解码</li>
</ol>
<p>对于二进制&#x2F;格雷编码的种群，我们要对其进行解码才能得到其表现型。</p>
<p>• bs2int (二进制&#x2F;格雷码转整数)</p>
<p>• bs2real (二进制&#x2F;格雷码转实数)</p>
<p>• bs2ri (二进制&#x2F;格雷码转实整数)</p>
<ol start="8">
<li>数据可视化</li>
</ol>
<p>• moeaplot (多目标优化目标空间绘图函数)</p>
<p>• soeaplot (单目标优化绘图函数)</p>
<p>• trcplot (进化记录器绘图函数)</p>
<p>• varplot (决策变量绘图函数)</p>
<p>如果是在iPython 控制台中调用可视化绘图函数（例如使用Spyder 开发工具），一般图像会默认显示在控制台中。此时可以在控制台下执行%matplotlib 来设置把图像显示在一个独立窗口中。</p>
<ol start="9">
<li>多目标相关</li>
</ol>
<p>• awGA (适应性权重法多目标聚合函数)</p>
<p>• rwGA (随机权重法多目标聚合函数)</p>
<p>• ndsortDED (基于排除法的帕累托层级划分)</p>
<p>• ndsortESS (基于ENS_SS 的快速非支配层级划分)</p>
<p>• ndsortTNS (基于T_ENS 的快速非支配层级划分)</p>
<p>• crtgp (创建在单位超空间中均匀的网格点集)</p>
<p>• crtup (创建在单位超平面内均匀分布的点集)</p>
<p>• crowdis (拥挤距离计算)</p>
<p>• refgselect (利用参考点引导的个体选择)</p>
<p>• refselect (基于参考点的“入龛”个体选择)</p>
<ol start="10">
<li>模板相关</li>
</ol>
<p>• soea_DE_best_1_bin_templet (差分进化DE&#x2F;best&#x2F;1&#x2F;bin 算法模板)</p>
<p>• soea_DE_best_1_L_templet (差分进化DE&#x2F;best&#x2F;1&#x2F;L 算法模板)</p>
<p>• soea_DE_rand_1_bin_templet (差分进化DE&#x2F;rand&#x2F;1&#x2F;bin 算法模板)</p>
<p>• soea_DE_rand_1_L_templet (差分进化DE&#x2F;rand&#x2F;1&#x2F;L 算法模板)</p>
<p>• soea_DE_targetToBest_1_bin_templet (差分进化DE&#x2F;targetToBest&#x2F;1&#x2F;bin 算法模板)</p>
<p>• soea_DE_targetToBest_1_L_templet (差分进化DE&#x2F;targetToBest&#x2F;1&#x2F;L 算法模板)</p>
<p>• soea_ES_1_plus_1_templet ((1+1) 进化策略模板)</p>
<p>• soea_EGA_templet (精英保留的遗传算法模板)</p>
<p>• soea_SEGA_templet (增强精英保留的遗传算法模板)</p>
<p>• soea_SGA_templet (最简单、最经典的遗传算法模板)</p>
<p>• soea_GGAP_SGA_templet (带代沟的简单遗传算法模板)</p>
<p>• soea_studGA_templet (种马遗传算法模板)</p>
<p>• soea_steady_GA_templet (稳态遗传算法模板)</p>
<p>• soea_psy_EGA_templet (精英保留的多染色体遗传算法模板)</p>
<p>• soea_psy_SEGA_templet (增强精英保留的多染色体遗传算法模板)</p>
<p>• soea_psy_SGA_templet (最简单、最经典的多染色体遗传算法模板)</p>
<p>• soea_psy_GGAP_SGA_templet (带代沟的多染色体简单遗传算法模板)</p>
<p>• soea_psy_studGA_templet (多染色体种马遗传算法模板)</p>
<p>• soea_psy_steady_GA_templet (多染色体稳态遗传算法模板)</p>
<p>• moea_awGA_templet (基于awGA 算法的多目标进化算法模板)</p>
<p>• moea_NSGA2_DE_templet (基于NSGA-II-DE 算法的多目标进化算法模板)</p>
<p>• moea_NSGA2_archive_templet (带全局存档的多目标进化NSGA-II 算法模板)</p>
<p>• moea_NSGA2_templet (基于NSGA-II 算法的多目标进化算法模板)</p>
<p>• moea_NSGA3_DE_templet (基于NSGA-III-DE 算法的多目标进化算法模板)</p>
<p>• moea_NSGA3_templet (基于NSGA-III 算法的多目标进化算法模板)</p>
<p>• moea_RVEA_templet (基于RVEA 算法的多目标进化算法模板)</p>
<p>• moea_RVEA_RES_templet (基于带参考点再生策略的RVEA 算法的多目标进化算法</p>
<p>模板)</p>
<p>• moea_psy_awGA_templet (基于awGA 算法的多染色体多目标进化算法模板)</p>
<p>• moea_psy_NSGA2_archive_templet (带全局存档的多染色体多目标进化NSGA-II 算</p>
<p>法模板)</p>
<p>• moea_psy_NSGA2_templet (基于NSGA-II 算法的多染色体多目标进化算法模板)</p>
<p>• moea_psy_NSGA3_templet (基于NSGA-III 算法的多染色体多目标进化算法模板)</p>
<p>• moea_psy_RVEA_templet (基于RVEA 算法的多染色体多目标进化算法模板)</p>
<p>• moea_psy_RVEA_RES_templet (基于带参考点再生策略的多染色体RVEA 算法的多</p>
<p>目标进化算法模板)</p>
<p>详细API文档可以在Python控制台中执行help(模块名)查看。</p>
<p>下面讲一下Geatpy中重要的数据结构：</p>
<p>Geatpy的大部分数据都是存储在numpy的array数组里的，numpy中另外还有matrix的矩阵类型，但我们不使用它，于是本文档默认array就是存储“矩阵”(也可以存储一维向量，接下来会谈到)。其中有一些细节需要特别注意：numpy的array在表示行向量时会有2种不同的结构，一种是1行n列的矩阵，它是二维的；一种是纯粹的一维行向量。因此，在Geatpy教程中会严格区分这两种概念，我们称前者为“行矩阵”，后者为“行向量”。Geatpy中不会使用超过二维的array。</p>
<p>例如有一个行向量x，其值为[1 2 3 4 5 6]，那么，用print(x.shape)输出其规格，可以得到(6,)，若x是行矩阵而不是行向量，那么x的规格就变成是(1,6)而不再是(6,)。</p>
<p>在numpy的array类型中，实际上没有“列向量”的概念。所谓“向量”是指一维的，但用numpy的array表示列向量时，它实际上是二维的，只不过只有1列。我们不纠结于这个细节，统一仍用“列向量”来称呼这种只有1列的矩阵。</p>
<p>在编程中，如果对numpy的array感到疑惑，你可以用”print(变量.shape)”语句来输出其维度信息，以确定其准确的维度。</p>
<ul>
<li>种群染色体的数据结构：</li>
</ul>
<p>Geatpy中，种群染色体是一个numpy的array类型的二维矩阵，一般用Chrom命名，每一行对应一个个体的一条染色体。若要采用多染色体，则可以创建多个相关联的Chrom即可。默认一个Chrom的一行对应的只有一条染色体。</p>
<p>我们一般把种群的规模(即种群的个体数)用Nind命名；把种群个体的染色体长度用$Lind$命名，则Chrom的结构如下所示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8C8C3299ADAE417EB1783D36578A5C51.png"></p>
<ul>
<li>种群表现型的数据结构：</li>
</ul>
<p>种群表现型的数据结构跟种群染色体基本一致，也是numpy的array类型。我们一般用Phen来命名。它是种群染色体矩阵Chrom经过解码后得到的基因表现型矩阵，每一行对应一个个体，每一列对应一个决策变量。若用Nvar表示变量的个数，则种群表现型矩阵Phen的结构如下图：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC16129D1032E479FA0EFD12EA1EC6CBC.png"></p>
<p>Phen的值与采用的解码方式有关。Geatpy提供二进制&#x2F;格雷码编码转十进制整数或实数的解码方式。另外，在Geatpy也可以使用不需要解码的“实值编码”种群，这种种群的染色体的每一位就对应着决策变量的实际值，即这种编码方式下Phen等价Chrom。</p>
<p>这里需要注意的是：我们可以用不同的方式去解码一个种群染色体，得到的结果往往是不同的。</p>
<ul>
<li>目标函数值的数据结构：</li>
</ul>
<p>Geatpy采用numpy的array类型矩阵来存储种群的目标函数值。一般命名为ObjV，每一行对应每一个个体，因此它拥有与Chrom相同的行数；每一列对应一个目标函数，因此对于单目标函数，ObjV会只有1列；而对于多目标函数，ObjV会有多列。例如ObjV 是一个二元函数值矩阵：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7A6B1C0182B44CC1A8F73EE0CB34746B.png"></p>
<p>其第一列就代表目标函数f1的目标函数值，第二列代表目标函数f2的目标函数值。</p>
<ul>
<li>个体适应度的数据结构：</li>
</ul>
<p>Geatpy采用列向量来存储种群个体适应度。一般命名为FitnV，它同样是numpy的array类型，每一行对应种群矩阵的每一个个体。因此它拥有与Chrom相同的行数。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F4991EB7AD8F44E239D4931FF1DEF4B0D.png"></p>
<p>Geatpy中的适应度遵循“最小适应度为0”的约定。</p>
<ul>
<li>个体违反约束程度的数据结构</li>
</ul>
<p>Geatpy采用Numpy array类型的矩阵CV(Constraint Violation Value)来存储种群个体违反各个约束条件的程度。一般命名为$CV$，它的每一行对应种群的每一个个体，因此它拥有与$Chrom$相同的行数；每一列对应一个约束条件，因此若有一个约束条件，那么CV矩阵就会只有一列，如有多个约束条件，CV矩阵就会有多列。如果设有$num$个约束，则CV矩阵的结构如下图所示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2754B90699E84B7FA7FB838449ACF86C.png"></p>
<p> CV矩阵的某个元素若小于或等于0，则表示该元素对应的个体满足对应的约束条件。若大于0，则表示违反约束条件，在大于0的条件下值越大，该个体违反该约束的程度就越高。Geatpy提供两种处理约束条件的方法，一种是罚函数法，另一种是可行性法则。在使用可行性法则处理约束条件时，需要用到CV矩阵。具体用法可详见后面两章中关于使用可行性法则来处理约束条件的相关说明。</p>
<ul>
<li>译码矩阵的数据结构：</li>
</ul>
<p>Geatpy使用译码矩阵(俗称区域描述器)来描述种群染色体的特征，如染色体中的每一位元素所表达的决策变量的范围、是否包含范围的边界、采用二进制还是格雷码、是否使用对数刻度、染色体解码后所代表的决策变量的是连续型变量还是离散型变量等等。</p>
<p>在只使用工具箱的库函数而不使用Geatpy提供的面向对象的进化算法框架时，译码矩阵可以单独使用。若采用Geatpy提供的面向对象的进化算法框架时，译码矩阵可以与一个存储着种群染色体编码方式的字符串Encoding来配合使用。目前Geatpy中有三种Encoding，分别为：</p>
<p>• ’BG’ (二进制&#x2F;格雷码)</p>
<p>• ’RI’ (实整数编码，即实数和整数的混合编码)</p>
<p>• ’P’ (排列编码，即染色体每一位的元素都是互异的)</p>
<p>这里有个小小的归类值得注意：’RI’和’P’编码的染色体都不需要解码，染色体上的每一位本身就代表着决策变量的真实值，因此“实整数编码”和“排列编码”可统称为“实值编码”。</p>
<ol>
<li>对于$Encoding &#x3D; ‘BG’$的种群，使用8行$n$列的矩阵FieldD来作为译码矩阵，$n$是染色体所表达的决策变量个数。FieldD的结构如下：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC1AF7C6CAE0F4E6DA089F9CED4E140F8.png"></p>
<p>lens, lb, ub, codes, scales, lbin, ubin, varTypes 均为长度等于决策变量个数的行向</p>
<p>量。</p>
<p>其中，lens 包含染色体的每个子染色体的长度。sum(lens) 等于染色体长度。</p>
<p>lb 和ub 分别代表每个决策变量的上界和下界。</p>
<p>codes 指明染色体子串用的是二进制编码还是格雷编码。codes[i] &#x3D; 0 表示第i 个变量使用的是标准二进制编码；codes[i] &#x3D; 1 表示使用格雷编码。</p>
<p>scales 指明每个子串用的是算术刻度还是对数刻度。scales[i] &#x3D; 0 为算术刻度，scales[i] &#x3D; 1 为对数刻度。对数刻度可以用于变量的范围较大而且不确定的情况，对于大范围的参数边界，对数刻度让搜索可用较少的位数，从而减少了遗传算法的计算量。（注意：当某个变量是对数刻度时，其取值范围中不能有0，即要么上下界都大于0，要么上下界都小于0。）</p>
<p>lbin 和ubin 指明了变量是否包含其范围的边界。0 表示不包含边界；1 表示包含边界。</p>
<p>varTypes 指明了决策变量的类型，元素为0 表示对应位置的决策变量是连续型变量；1 表示对应的是离散型变量。</p>
<p>例如：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F29E05B3BF17D46C1AF69A5F5271DD891.png"></p>
<p>它表示待解码的种群染色体矩阵Chrom解码后可以表示成3个决策变量，每个决策变量的取值范围分别是[1,10], [2,9], [3,15]。其中第一第二个变量采用的是二进制编码，第三个变量采用的是格雷编码，且第一、第三个决策变量为连续型变量；第二个为离散型遍历。若Chrom为:</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCEDA91B6C5194A5998DAE5DF6E3DA5C5.png"></p>
<p>则可以执行以下语句进行解码：</p>
<ol>
<li><p>import geatpy as ea</p>
</li>
<li><p>Phen &#x3D; ea.bs2ri(Chrom, FieldD)</p>
</li>
</ol>
<p>解码后得到的种群表现型矩阵为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FFDCF89CDCBE94F09B8173AC130A85ECF.png"></p>
<ol start="2">
<li>对于实值编码(即前面所说的不需要解码的编码方式) 的种群，使用3 行n 列的矩阵FieldDR 来作为译码矩阵，n 是染色体所表达的控制变量个数。FieldDR 的结构如下：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F12FBD82AD00744F7A2CDBF4CD2D74972.png"></p>
<p>这种结构的译码矩阵适用于Encoding 为’RI’(实整数编码) 和’P’(排列编码) 的种群染色体的解码。其中’P’(排列编码) 的译码矩阵会稍微有一点特殊之处：它要求FieldDR的第一行所有元素都相等，第二行所有元素也都相等，且第三行元素均为1(这是因为排列编码本身变量是离散的)。此时若记FieldDR 有Lind 列(即染色体长度为Lind)，则要求上界- 下界+ 1 ≥ Lind。例如：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F91A4F40285E6493A9523F751C7022F23.png"></p>
<p>它若是作为排列编码种群的译码矩阵，则表示种群染色体是由集合{2,3,4,5,6,7,8,9,10}中任意抽取7 个数出来的全排列，比如Chrom 为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1CD65E2A967043FA87D6FA6C48A9BD99.png"></p>
<p>上面的FieldD 和FieldDR 都是numpy 的array 类型，可统称为“Field”。可以直接用代码创建，例如：</p>
<ol>
<li><p>import numpy as np</p>
</li>
<li><p>FieldDR&#x3D;np.array([[-3, -4, 0, 2],</p>
</li>
<li><p>[ 2, 3, 2, 2],</p>
</li>
<li><p>[ 0, 0, 0, 0]])</p>
</li>
</ol>
<p>也可以用Geatpy 内置的crtfld 函数来方便地快速生成区域描述器，其详细用法可执行help(crtfld) 或查看API 文档。</p>
<ul>
<li>进化追踪器的数据结构：</li>
</ul>
<p>在使用Geatpy 进行进化算法编程时，常常建立一个进化追踪器(如pop_trace) 来记录种群在进化的过程中各代的最优个体，尤其是采用无精英保留机制时，进化追踪器帮助我们记录种群在进化的“历史长河”中产生过的最优个体。待进化完成后，再从进化追踪器中挑选出“历史最优”的个体。这种进化记录器有多种，其中一种是numpy 的array 类型的，结构如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8BD6F61C10FE4A79818EC89CAF05E430.png"></p>
<p>其中MAXGEN 是种群进化的代数。trace 的每一列代表不同的指标，比如第一列记录各代种群的最佳目标函数值，第二列记录各代种群的平均目标函数值……trace 的每一行对应每一代，如第一行代表第一代，第二行代表第二代……</p>
<p>另外一种进化记录器是一个列表，列表中的每一个元素都是一个拥有相同数据类型的数据。比如在Geatpy 的面向对象进化算法框架中的pop_trace，它是一个列表，列表中的每一个元素都是历代的种群对象。</p>
<ul>
<li>种群结构</li>
</ul>
<p>1） Population 类</p>
<p>在Geatpy 提供的面向对象进化算法框架中，种群类(Population) 是一个存储着与种群个体相关信息的类。它有以下基本属性：</p>
<p>sizes    : int   - 种群规模，即种群的个体数目。</p>
<p>           Lind     : int   - 种群染色体长度。</p>
<p>           Encoding : str   - 染色体编码方式。</p>
<p>           Field    : array - 译码矩阵，可以是FieldD或FieldDR。</p>
<p>           Chrom    : array - 种群染色体矩阵，每一行对应一个个体的一条染色体。</p>
<p>           ObjV     : array - 种群目标函数值矩阵。</p>
<p>           FitnV    : array - 种群个体适应度列向量。</p>
<p>           CV       : array - 种群个体违反约束条件程度的矩阵。</p>
<p>           Phen     : array - 种群表现型矩阵。</p>
<p>可以直接对种群对象进行提取个体、个体合并等操作，比如pop1和pop2是两个种群对象，则通过语句“pop3 &#x3D; pop1 + pop2”，即可把两个种群的个体合并，得到一个新的种群。在合并的过程中，实际上是把种群的各个属性进行合并，然后用合并的数据来生成一个新的种群(详见Population.py)。又比如执行语句“pop3 &#x3D; pop1[[0]]”，可以把种群的第0号个体抽取出来，得到一个新的只有一个个体的种群对象pop3。值得注意的是，种群的这种个体抽取操作要求下标必须为列表或是Numpy array类型的行向量，不能是标量(详见Population.py)。</p>
<p>易错注意：在Geatpy中，必要地对种群对象的这些成员属性进行合法性检查是必要的，但过多的检查会在一定程度上降低框架的性能。其中最容易使得种群对象成员属性出现异常的地方在于目标函数值矩阵ObjV以及CV矩阵的生成。在Geatpy中，ObjV和CV是在Problem问题类的目标函数接口aimFunc()中计算生成的，无论中间过程它们具体是如何计算的，计算得到的结果必须满足：ObjV和CV都是Numpy array类型矩阵，且行数等于种群的个体数目。ObjV的每一行对应一个个体，每一列对应一个优化目标。CV矩阵的每一行也是对应一个个体，而每一列对应一个约束条件。根据Geatpy数据结构可知，种群对象中的Chrom、ObjV、FitnV、CV和Phen都是Numpy array类型的行数等于种群规模sizes的矩阵，即这些成员属性的每一行都跟种群的每一个个体是一一对应的。Geatpy框架在运行过程中所抛出大多数异常都是由于这些变量不合法所致。此时可以使用“shape”来输出变量的维度信息，比如：</p>
<ol>
<li><p>print(pop.sizes)</p>
</li>
<li><p>print(pop.Chrom.shape)</p>
</li>
<li><p>print(pop.ObjV.shape)</p>
</li>
<li><p>print(pop.CV.shape)</p>
</li>
</ol>
<p>其中pop为一个种群对象。</p>
<p>2）PsyPopulation类</p>
<p>PsyPopulation类是Population的子类，它提供Population类所不支持的多染色体混合编码。它有以下基本属性：</p>
<p>sizes     : int           - 种群规模，即种群的个体数目。</p>
<p>           ChromNum  : int           - 染色体的数目，即每个个体有多少条染色体。</p>
<p>           Linds     : list     - 存储种群各染色体长度的列表。</p>
<p>           Encodings : list  - 存储各染色体编码方式的列表。</p>
<p>           Fields    : list   - 存储各染色体对应的译码矩阵的列表。</p>
<p>           Chroms    : list   - 存储种群各染色体矩阵的列表。</p>
<p>           ObjV      : array         - 种群目标函数值矩阵。</p>
<p>           FitnV     : array         - 种群个体适应度列向量。</p>
<p>           CV        : array         - 种群个体违反约束条件程度的矩阵。</p>
<p>           Phen      : array         - 种群表现型矩阵。</p>
<p>可见PsyPopulation类基本与Population类一样，不同之处是采用Linds、Encodings、Fields和Chroms分别存储多个Lind、Encoding、Field和Chrom。</p>
<p>PsyPopulation类的对象往往与带“psy”字样的进化算法模板配合使用，以实现多染色体混合编码的进化优化。</p>
<p>在后面我将详细阐述如何使用Geatpy提供的面向对象进化算法框架的算法模板来实现使用进化算法求解问题。</p>
<p>欢迎继续跟进，感谢！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(4%E6%9D%A1%E6%B6%88%E6%81%AF)Python%E9%81%97%E4%BC%A0%E5%92%8C%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%88%E4%B8%80%EF%BC%89Geatpy%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8_jazzbin%E7%9A%84%E5%8D%9A%E5%AE%A2-CSDN%E5%8D%9A%E5%AE%A2_geatpy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(4%E6%9D%A1%E6%B6%88%E6%81%AF)Python%E9%81%97%E4%BC%A0%E5%92%8C%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%88%E4%B8%80%EF%BC%89Geatpy%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8_jazzbin%E7%9A%84%E5%8D%9A%E5%AE%A2-CSDN%E5%8D%9A%E5%AE%A2_geatpy/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:49" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:49+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-12-17 04:45:04" itemprop="dateModified" datetime="2022-12-17T04:45:04+08:00">2022-12-17</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Geatpy是一个高性能实用型的Python遗传算法工具箱，提供一个面向对象的进化算法框架，经过全面改版后，新版Geatpy2目前由华南农业大学、暨南大学、华南理工等本硕博学生联合团队开发及维护。</p>
<ul>
<li><p>Website (including documentation): <a target="_blank" rel="noopener" href="http://www.geatpy.com/">http://www.geatpy.com</a></p>
</li>
<li><p>Demo : <a target="_blank" rel="noopener" href="https://github.com/geatpy-dev/geatpy/tree/master/geatpy/demo">https://github.com/geatpy-dev/geatpy/tree/master/geatpy/demo</a></p>
</li>
<li><p>Pypi page : <a target="_blank" rel="noopener" href="https://pypi.org/project/geatpy/">https://pypi.org/project/geatpy/</a></p>
</li>
<li><p>Contact us: <a target="_blank" rel="noopener" href="http://geatpy.com/index.php/about/">http://geatpy.com/index.php/about/</a></p>
</li>
<li><p>Bug reports: <a target="_blank" rel="noopener" href="https://github.com/geatpy-dev/geatpy/issues">https://github.com/geatpy-dev/geatpy/issues</a></p>
</li>
<li><p>Notice: <a target="_blank" rel="noopener" href="http://geatpy.com/index.php/notice/">http://geatpy.com/index.php/notice/</a></p>
</li>
<li><p>FAQ: <a target="_blank" rel="noopener" href="http://geatpy.com/index.php/faq/">http://geatpy.com/index.php/faq/</a></p>
</li>
</ul>
<p>Geatpy提供了许多已实现的遗传和进化算法相关算子的库函数，如初始化种群、选择、交叉、变异、重插入、多目标优化非支配排序等，并且提供诸多已实现的进化算法模板来实现多样化的进化算法。其执行效率高于Matlab、Java和Python编写的一些知名工具箱、平台或框架等，学习成本低、模块高度脱耦、扩展性高。</p>
<p>Geatpy支持二进制&#x2F;格雷码编码种群、实数值种群、整数值种群、排列编码种群。支持轮盘赌选择、随机抽样选择、锦标赛选择。提供单点交叉、两点交叉、洗牌交叉、部分匹配交叉(PMX)、顺序交叉(OX)、线性重组、离散重组、中间重组等重组算子。提供简单离散变异、实数值变异、整数值变异、互换变异等变异算子。支持随机重插入、精英重插入。支持awGA、rwGA、nsga2、快速非支配排序等多目标优化的库函数、提供进化算法框架下的常用进化算法模板等。</p>
<p>关于遗传算法、进化算法的学习资料，在官网中<a target="_blank" rel="noopener" href="https://www.geatpy.com/">https://www.geatpy.com</a> 有详细讲解以及相关的学术论文链接。同时网上也有很多资料。</p>
<p>闲话少说……下面讲一下怎么安装和使用：</p>
<p>先说一下安装方法：</p>
<p>首先是要windows系统，Python要是3.5，3.6或3.7版本 ，并且安装了pip。只需在控制台执行</p>
<p>pip install geatpy</p>
<p>即可安装成功。或者到github上下载源码进行编译安装：<a target="_blank" rel="noopener" href="https://github.com/geatpy-dev/geatpy">https://github.com/geatpy-dev/geatpy</a> 。推荐是直接用pip的方式安装。因为这样方便后续的更新。我为了方便运行demo代码以及查看源码和官方教程文档，因此另外在github上也下载了（但仍用pip方式安装）。</p>
<p>有些初学Python的读者反映还是不知道怎么安装，或者安装之后不知道怎么写代码。这里推荐安装Anaconda，它集成了Python的许多常用的运行库，比如Numpy、Scipy等。其内置的Spyder开发软件的风格跟Matlab类似，给人熟悉的感觉，更容易上手。</p>
<p>再说一下更新方法：</p>
<p>Geatpy在持续更新。可以通过以下命令使电脑上的版本与官方最新版保持一致：</p>
<p>pip install –upgrade geatpy</p>
<p>若在更新过程中遇到”–user”错误的问题，这是用pip进行安装时遇到的常见问题之一。意味着需要以管理员方式运行：</p>
<p>pip install –user –upgrade geatpy</p>
<p>Geatpy提供2种方式来使用进化算法求解问题。先来讲一下第一种最基本的实现方式：编写编程脚本。</p>
<ol>
<li>编写脚本实现遗传算法：</li>
</ol>
<p>以一个非常简单的单目标优化问题为例：求f(x)&#x3D;x<em>sin(10</em>pi*x)+2.0 在 x∈[-1,2] 上的最大值。</p>
<p>直接编写脚本如下：</p>
<ol>
<li><p>“””demo.py”””</p>
</li>
<li><p>import numpy as np</p>
</li>
<li><p>import geatpy as ea # 导入geatpy库</p>
</li>
<li><p>import matplotlib.pyplot as plt</p>
</li>
<li><p>import time</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;目标函数&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>defaim(x):# 传入种群染色体矩阵解码后的基因表现型矩阵</p>
</li>
<li><p>return x * np.sin(10 * np.pi * x) + 2.0</p>
</li>
<li><p>x &#x3D; np.linspace(-1, 2, 200)</p>
</li>
<li><p>plt.plot(x, aim(x)) # 绘制目标函数图像</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;变量设置&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>x1 &#x3D; [-1, 2]                   # 自变量范围</p>
</li>
<li><p>b1 &#x3D; [1, 1]                    # 自变量边界</p>
</li>
<li><p>varTypes &#x3D; np.array([0])       # 自变量的类型，0表示连续，1表示离散</p>
</li>
<li><p>Encoding &#x3D; ‘BG’# ‘BG’表示采用二进制&#x2F;格雷编码</p>
</li>
<li><p>codes &#x3D; [1]                    # 变量的编码方式，2个变量均使用格雷编码</p>
</li>
<li><p>precisions &#x3D;[4]                # 变量的编码精度</p>
</li>
<li><p>scales &#x3D; [0]                   # 采用算术刻度</p>
</li>
<li><p>ranges&#x3D;np.vstack([x1]).T       # 生成自变量的范围矩阵</p>
</li>
<li><p>borders&#x3D;np.vstack([b1]).T      # 生成自变量的边界矩阵</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;遗传算法参数设置&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>NIND &#x3D; 40;                     # 种群个体数目</p>
</li>
<li><p>MAXGEN &#x3D; 25;                   # 最大遗传代数</p>
</li>
<li><p>FieldD &#x3D; ea.crtfld(Encoding,varTypes,ranges,borders,precisions,codes,scales) # 调用函数创建区域描述器</p>
</li>
<li><p>Lind &#x3D; int(np.sum(FieldD[0, :]))          # 计算编码后的染色体长度</p>
</li>
<li><p>obj_trace &#x3D; np.zeros((MAXGEN, 2))         # 定义目标函数值记录器</p>
</li>
<li><p>var_trace &#x3D; np.zeros((MAXGEN, Lind))      # 定义染色体记录器，记录每一代最优个体的染色体</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;开始遗传算法进化&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>start_time &#x3D; time.time()                             # 开始计时</p>
</li>
<li><p>Chrom &#x3D; ea.crtbp(NIND, Lind)                         # 生成种群染色体矩阵</p>
</li>
<li><p>variable &#x3D; ea.bs2real(Chrom, FieldD)                 # 对初始种群进行解码</p>
</li>
<li><p>ObjV &#x3D; aim(variable)                                 # 计算初始种群个体的目标函数值</p>
</li>
<li><p>best_ind &#x3D; np.argmax(ObjV)                           # 计算当代最优个体的序号</p>
</li>
<li><h1 id="开始进化"><a href="#开始进化" class="headerlink" title="开始进化"></a>开始进化</h1></li>
<li><p>for gen in range(MAXGEN):</p>
</li>
<li><pre><code>FitnV = ea.ranking(-ObjV)                        # 根据目标函数大小分配适应度值(由于遵循目标最小化约定，因此最大化问题要对目标函数值乘上-1)
</code></pre>
</li>
<li><pre><code>SelCh=Chrom[ea.selecting(&#39;rws&#39;, FitnV, NIND-1), :] # 选择，采用&#39;rws&#39;轮盘赌选择
</code></pre>
</li>
<li><pre><code>SelCh=ea.recombin(&#39;xovsp&#39;, SelCh, 0.7)           # 重组(采用两点交叉方式，交叉概率为0.7)
</code></pre>
</li>
<li><pre><code>SelCh=ea.mutbin(Encoding, SelCh)                 # 二进制种群变异
</code></pre>
</li>
<li><h1 id="把父代精英个体与子代合并"><a href="#把父代精英个体与子代合并" class="headerlink" title="把父代精英个体与子代合并"></a>把父代精英个体与子代合并</h1></li>
<li><pre><code>Chrom = np.vstack([Chrom[best_ind, :], SelCh])
</code></pre>
</li>
<li><pre><code>variable = ea.bs2real(Chrom, FieldD)             # 对育种种群进行解码(二进制转十进制)
</code></pre>
</li>
<li><pre><code>ObjV = aim(variable)                             # 求育种个体的目标函数值
</code></pre>
</li>
<li><h1 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h1></li>
<li><pre><code>best_ind = np.argmax(ObjV)                       # 计算当代最优个体的序号
</code></pre>
</li>
<li><pre><code>obj_trace[gen, 0] = np.sum(ObjV) / NIND          # 记录当代种群的目标函数均值
</code></pre>
</li>
<li><pre><code>obj_trace[gen, 1] = ObjV[best_ind]               # 记录当代种群最优个体目标函数值
</code></pre>
</li>
<li><pre><code>var_trace[gen, :] = Chrom[best_ind, :]           # 记录当代种群最优个体的变量值
</code></pre>
</li>
<li><h1 id="进化完成"><a href="#进化完成" class="headerlink" title="进化完成"></a>进化完成</h1></li>
<li><p>end_time &#x3D; time.time() # 结束计时</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;输出结果及绘图&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>best_gen &#x3D; np.argmax(obj_trace[:, [1]])</p>
</li>
<li><p>print(‘目标函数最大值：’, obj_trace[best_gen, 1]) # 输出目标函数最大值</p>
</li>
<li><p>variable &#x3D; ea.bs2real(var_trace[[best_gen], :], FieldD) # 解码得到表现型</p>
</li>
<li><p>print(‘对应的决策变量值为：’)</p>
</li>
<li><p>print(variable[0][0]) # 因为此处variable是一个矩阵，因此用[0][0]来取出里面的元素</p>
</li>
<li><p>print(‘用时：’, end_time - start_time)</p>
</li>
<li><p>plt.plot(variable, aim(variable),’bo’)</p>
</li>
<li><p>ea.trcplot(obj_trace, [[‘种群个体平均目标函数值’, ‘种群最优个体目标函数值’]])</p>
</li>
</ol>
<p>运行结果如下：</p>
<p>目标函数最大值： 3.850272716105895</p>
<p>对应的决策变量值为：</p>
<p>1.8505813776055176</p>
<p>用时： 0.02496051788330078</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD0AF59D7AE2F494E8AE53C186410C453.png"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F51C1148D1C2C40F29099AC0748E2800C.png"></p>
<p>仔细查看上述代码，我们会发现Geatpy的书写风格与Matlab大同小异，有Matlab相关编程经验的基本上可以无缝转移到Python上利用Geatpy进行遗传算法程序开发。</p>
<p>Geatpy提供了详尽的API文档，比如要查看上面代码中的”ranking”函数是干什么的，可以在python中执行</p>
<ol>
<li><p>import geatpy as ga</p>
</li>
<li><p>help(ga.ranking)</p>
</li>
</ol>
<p>即可看到”ranking”函数的相关使用方法。</p>
<p>另外官网上也有更多详尽的Geatpy教程：<a target="_blank" rel="noopener" href="http://geatpy.com/index.php/details/">http://geatpy.com/index.php/details/</a></p>
<ol start="2">
<li>利用框架实现遗传算法。</li>
</ol>
<p>Geatpy提供开放的面向对象进化算法框架。即“问题类”+“进化算法模板类+种群类”。对于一些复杂的进化算法，如多目标进化优化、改进的遗传算法等，按照上面所说的编写脚本代码是非常麻烦的，而用框架的方法可以极大提高编程效率。</p>
<p>这里给出一个利用框架实现NSGA-II算法求多目标优化函数ZDT-1的帕累托前沿面的例子：</p>
<p>第一步：首先编写ZDT1的问题类，写在“MyProblem.py”文件中：</p>
<ol>
<li><h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1></li>
<li><p>“””MyProblem.py”””</p>
</li>
<li><p>import numpy as np</p>
</li>
<li><p>import geatpy as ea</p>
</li>
<li><p>classMyProblem(ea.Problem):# 继承Problem父类</p>
</li>
<li><p>def__init__(self):</p>
</li>
<li><pre><code>    name = &#39;ZDT1&#39;# 初始化name（函数名称，可以随意设置）
</code></pre>
</li>
<li><pre><code>    M = 2# 初始化M（目标维数）
</code></pre>
</li>
<li><pre><code>    maxormins = [1] * M # 初始化maxormins（目标最小最大化标记列表，1：最小化该目标；-1：最大化该目标）
</code></pre>
</li>
<li><pre><code>    Dim = 30# 初始化Dim（决策变量维数）
</code></pre>
</li>
<li><pre><code>    varTypes = [0] * Dim # 初始化varTypes（决策变量的类型，0：实数；1：整数）
</code></pre>
</li>
<li><pre><code>    lb = [0] * Dim # 决策变量下界
</code></pre>
</li>
<li><pre><code>    ub = [1] * Dim # 决策变量上界
</code></pre>
</li>
<li><pre><code>    lbin = [1] * Dim # 决策变量下边界
</code></pre>
</li>
<li><pre><code>    ubin = [1] * Dim # 决策变量上边界
</code></pre>
</li>
<li><h1 id="调用父类构造方法完成实例化"><a href="#调用父类构造方法完成实例化" class="headerlink" title="调用父类构造方法完成实例化"></a>调用父类构造方法完成实例化</h1></li>
<li><pre><code>    ea.Problem.__init__(self, name, M, maxormins, Dim, varTypes, lb, ub, lbin, ubin)
</code></pre>
</li>
<li><p>defaimFunc(self, pop):# 目标函数</p>
</li>
<li><pre><code>    Vars = pop.Phen # 得到决策变量矩阵
</code></pre>
</li>
<li><pre><code>    ObjV1 = Vars[:, 0]
</code></pre>
</li>
<li><pre><code>    gx = 1 + 9 * np.sum(Vars[:, 1:30], 1)
</code></pre>
</li>
<li><pre><code>    hx = 1 - np.sqrt(ObjV1 / gx)
</code></pre>
</li>
<li><pre><code>    ObjV2 = gx * hx
</code></pre>
</li>
<li><pre><code>    pop.ObjV = np.array([ObjV1, ObjV2]).T # 把结果赋值给ObjV
</code></pre>
</li>
<li><p>defcalReferObjV(self):# 计算全局最优解作为目标函数参考值</p>
</li>
<li><pre><code>    N = 10000# 生成10000个参考点
</code></pre>
</li>
<li><pre><code>    ObjV1 = np.linspace(0, 1, N)
</code></pre>
</li>
<li><pre><code>    ObjV2 = 1 - np.sqrt(ObjV1)
</code></pre>
</li>
<li><pre><code>    globalBestObjV = np.array([ObjV1, ObjV2]).T
</code></pre>
</li>
<li><p>return globalBestObjV</p>
</li>
</ol>
<p>上面代码中，问题类的构造函数__init__()是用于定义与ZDT1测试问题相关的一些参数，如决策变量范围、类型、边界等等。aimFunc()是待优化的目标函数。calReferObjV()用来计算理论的全局最优解，这个理论最优解可以是通过计算得到的，也可以是通过导入外部文件的数据得到的，如果待求解的问题没有或尚不知道理论最优解是多少，则这个calReferObjV()函数可以省略不写。</p>
<p>第二步：在同一个文件夹下编写执行脚本，实例化上述问题类的对象，然后调用Geatpy提供的nsga2算法的进化算法模板(moea_NSGA2_templet)，最后结合理论全局最优解PF（即俗称的“真实前沿点”）通过计算GD、IGD、HV等指标来分析优化效果：</p>
<ol>
<li><h1 id="coding-utf-8-1"><a href="#coding-utf-8-1" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1></li>
<li><p>import geatpy as ea # import geatpy</p>
</li>
<li><p>from MyProblem import MyProblem</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;实例化问题对象&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>problem &#x3D; MyProblem()     # 生成问题对象</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;种群设置&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>Encoding &#x3D; ‘RI’# 编码方式</p>
</li>
<li><p>NIND &#x3D; 50# 种群规模</p>
</li>
<li><p>Field &#x3D; ea.crtfld(Encoding, problem.varTypes, problem.ranges, problem.borders) # 创建区域描述器</p>
</li>
<li><p>population &#x3D; ea.Population(Encoding, Field, NIND) # 实例化种群对象（此时种群还没被初始化，仅仅是完成种群对象的实例化）</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;算法参数设置&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>myAlgorithm &#x3D; ea.moea_NSGA2_templet(problem, population) # 实例化一个算法模板对象&#96;</p>
</li>
<li><p>myAlgorithm.MAXGEN &#x3D; 200# 最大进化代数</p>
</li>
<li><p>myAlgorithm.drawing &#x3D; 1# 设置绘图方式（0：不绘图；1：绘制结果图；2：绘制过程动画）</p>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;调用算法模板进行种群进化&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
</li>
<li><p>调用run执行算法模板，得到帕累托最优解集NDSet。NDSet是一个种群类Population的对象。</p>
</li>
<li><p>NDSet.ObjV为最优解个体的目标函数值；NDSet.Phen为对应的决策变量值。</p>
</li>
<li><p>详见Population.py中关于种群类的定义。</p>
</li>
<li><p>“””</p>
</li>
<li><p>NDSet &#x3D; myAlgorithm.run() # 执行算法模板，得到非支配种群</p>
</li>
<li><p>NDSet.save()              # 把结果保存到文件中</p>
</li>
<li><h1 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h1></li>
<li><p>print(‘用时：%f 秒’%(myAlgorithm.passTime))</p>
</li>
<li><p>print(‘评价次数：%d 次’%(myAlgorithm.evalsNum))</p>
</li>
<li><p>print(‘非支配个体数：%d 个’%(NDSet.sizes))</p>
</li>
<li><p>print(‘单位时间找到帕累托前沿点个数：%d 个’%(int(NDSet.sizes &#x2F;&#x2F; myAlgorithm.passTime)))</p>
</li>
<li><h1 id="计算指标"><a href="#计算指标" class="headerlink" title="计算指标"></a>计算指标</h1></li>
<li><p>PF &#x3D; problem.getReferObjV() # 获取真实前沿，详见Problem.py中关于Problem类的定义</p>
</li>
<li><p>if PF isnotNoneand NDSet.sizes !&#x3D; 0:</p>
</li>
<li><pre><code>GD = ea.indicator.GD(NDSet.ObjV, PF)       # 计算GD指标
</code></pre>
</li>
<li><pre><code>IGD = ea.indicator.IGD(NDSet.ObjV, PF)     # 计算IGD指标
</code></pre>
</li>
<li><pre><code>HV = ea.indicator.HV(NDSet.ObjV, PF)       # 计算HV指标
</code></pre>
</li>
<li><pre><code>Spacing = ea.indicator.Spacing(NDSet.ObjV) # 计算Spacing指标
</code></pre>
</li>
<li><pre><code>print(&#39;GD&#39;,GD)
</code></pre>
</li>
<li><pre><code>print(&#39;IGD&#39;,IGD)
</code></pre>
</li>
<li><pre><code>print(&#39;HV&#39;, HV)
</code></pre>
</li>
<li><pre><code>print(&#39;Spacing&#39;, Spacing)
</code></pre>
</li>
<li><p>“””&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;进化过程指标追踪分析&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;”””</p>
</li>
<li><p>if PF isnotNone:</p>
</li>
<li><pre><code>metricName = [[&#39;IGD&#39;], [&#39;HV&#39;]]
</code></pre>
</li>
<li><pre><code>[NDSet_trace, Metrics] = ea.indicator.moea_tracking(myAlgorithm.pop_trace, PF, metricName, problem.maxormins)
</code></pre>
</li>
<li><h1 id="绘制指标追踪分析图"><a href="#绘制指标追踪分析图" class="headerlink" title="绘制指标追踪分析图"></a>绘制指标追踪分析图</h1></li>
<li><pre><code>ea.trcplot(Metrics, labels = metricName, titles = metricName)
</code></pre>
</li>
</ol>
<p>运行结果如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7E821BCC8F5B4C3EB91257A293A772BF.png"></p>
<p>种群信息导出完毕。</p>
<p>用时：0.503653 秒</p>
<p>评价次数：10000 次</p>
<p>非支配个体数：50 个</p>
<p>单位时间找到帕累托前沿点个数：99 个</p>
<p>GD 0.0011025023611967554</p>
<p>IGD 0.15098973339777405</p>
<p>HV 0.624906599521637</p>
<p>Spacing 0.009326105831814594</p>
<p>正在进行进化追踪指标分析，请稍后……</p>
<p>指标追踪分析结束，进化记录器中有效进化代数为: 200</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF6E9BC0E49D44B6D873807DC46D0D008.png"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA07567D6AD7B466794A5D0A27595A546.png"></p>
<p>上述代码中已经对各个流程进行了详细的注释。其中进化算法的核心逻辑是写在进化算法模板内部的，可前往查看对应的源代码。此外，我们还可以参考Geatpy进化算法模板的源代码来自定义算法模板，以实现丰富多样的进化算法，如各种各样的改进的进化算法等：</p>
<p>最后值得注意的是：目标函数aimFunc()那一块地方最容易写错。aimFunc()的输入参数pop是一个种群对象（有关种群对象可以查看工具箱中的Population.py类源码，或者查看Geatpy数据结构）。pop.Phen是种群的表现型矩阵，意思是种群染色体解码后得到的表现型矩阵，它对应的即为问题类中的决策变量。Phen是一个矩阵，每一行对应种群中的一个个体的表现型。在计算目标函数时，可以把这个Phen拆成一行一行，即逐个逐个个体地计算目标函数值，然后再拼成一个矩阵赋值给pop对象的ObjV属性。也可以利用Numpy的矩阵化计算来“一口气”把种群所有个体的目标函数值计算出来。无论采用的是哪种计算方法，最后得到的目标函数值是要保存在pop对象的ObjV属性中的，这个ObjV是“种群目标函数值矩阵”，每一行对应一个个体的所有目标函数值，每一列对应一个目标。比如：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F218132B7BE4E4DD295513C7F7D23889B.png"></p>
<p>它表示有种群3个个体，待优化目标有2个。</p>
<p>后面的博客将深入理解Geatpy的数据结构、进化算法框架的用法与扩展，以及探讨框架的核心——进化算法模板的实现。还会讲一些使用Geatpy解决问题的案例。欢迎继续跟进~感谢！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(33)aurora%E7%BC%96%E8%BE%91%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89_%E6%8B%82%E6%B0%B4%E9%A3%98%E7%BB%B5_%E6%96%B0%E6%B5%AA%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/013_%E5%AD%A6%E4%B9%A0/(33)aurora%E7%BC%96%E8%BE%91%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9C%AD%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89_%E6%8B%82%E6%B0%B4%E9%A3%98%E7%BB%B5_%E6%96%B0%E6%B5%AA%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:49" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:49+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-04 05:24:27" itemprop="dateModified" datetime="2022-11-04T05:24:27+08:00">2022-11-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83%EF%BC%88Beta%20Distribution%EF%BC%89%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%20%20%20%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%80%85%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99(Datalearner)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83%EF%BC%88Beta%20Distribution%EF%BC%89%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%85%B6%E5%BA%94%E7%94%A8%20%20%20%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%80%85%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99(Datalearner)/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:24" itemprop="dateModified" datetime="2022-11-07T06:06:24+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>贝塔分布（Beta Distribution）简介及其应用</p>
<p>贝塔分布（Beta Distribution）是一个连续的概率分布，它只有两个参数。它最重要的应用是为某项实验的成功概率建模。在本篇博客中，我们使用Beta分布作为描述。</p>
<ul>
<li><p>一、Beta分布的定义及其简介</p>
</li>
<li><p>二、为实验成功概率建模（为棒球运动员的击球率建模）</p>
</li>
<li><p>三、为顺序统计量建模</p>
</li>
<li><p>四、旧货商服务质量推断</p>
</li>
</ul>
<p>一、Beta分布的定义及其简介</p>
<p>Beta分布是一个定义在 [0,1] 区间上的连续概率分布族，它有两个正值参数，称为形状参数，一般用 α 和 β 表示。在贝叶斯推断中，Beta分布是Bernoulli、二项分布、负二项分布和几何分布的共轭先验分布。Beta分布的概率密度函数形式如下：</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5fc18388353b219c482e8e35ca4aae808ab1be81"></p>
<p>  这里的 Γ 表示gamma函数。</p>
<p>Beta分布的均值是：​α&#x2F;(β+​​α​​)​​</p>
<p>方差是：​αβ​​&#x2F;((α+β)​²(α+β+1)​​)</p>
<p>下面我们看一下Beta分布的图形：</p>
<p>beta分布的R语言实例，首先，我们可以画一个beta分布的概率密度函数。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set.<span class="title function_">seed</span>(<span class="number">1</span>)</span><br><span class="line">x&lt;-<span class="title function_">seq</span>(<span class="number">0</span>,<span class="number">1</span>,length.<span class="property">out</span>=<span class="number">10000</span>)</span><br><span class="line"><span class="title function_">plot</span>(<span class="number">0</span>,<span class="number">0</span>,main=<span class="string">&#x27;probability density function&#x27;</span>,xlim=<span class="title function_">c</span>(<span class="number">0</span>,<span class="number">1</span>),ylim=<span class="title function_">c</span>(<span class="number">0</span>,<span class="number">2.5</span>),ylab=<span class="string">&#x27;PDF&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">0.5</span>,<span class="number">0.5</span>),col=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">1</span>,<span class="number">2</span>),col=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">2</span>,<span class="number">2</span>),col=<span class="string">&#x27;pink&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">2</span>,<span class="number">5</span>),col=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">1</span>,<span class="number">3</span>),col=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">dbeta</span>(x,<span class="number">5</span>,<span class="number">1</span>),col=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"><span class="title function_">legend</span>(<span class="string">&#x27;top&#x27;</span>,legend=<span class="title function_">c</span>(<span class="string">&#x27;α=0.5,β=0.5&#x27;</span>,<span class="string">&#x27;α=1,β=2&#x27;</span>,<span class="string">&#x27;α=2,β=2&#x27;</span>,<span class="string">&#x27;α=2,β=5&#x27;</span>,<span class="string">&#x27;α=1,β=3&#x27;</span>,<span class="string">&#x27;α=5,β=1&#x27;</span>),col=<span class="title function_">c</span>(<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;pink&#x27;</span>,<span class="string">&#x27;orange&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>,<span class="string">&#x27;black&#x27;</span>),lwd=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F17D94671A49441AD89551AC910FA958A.jpeg"></p>
<p>　　我们再来画一个beta分布的累计概率密度函数</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">set.<span class="title function_">seed</span>(<span class="number">1</span>)</span><br><span class="line">x&lt;-<span class="title function_">seq</span>(<span class="number">0</span>,<span class="number">1</span>,length.<span class="property">out</span>=<span class="number">10000</span>)</span><br><span class="line"><span class="title function_">plot</span>(<span class="number">0</span>,<span class="number">0</span>,main=<span class="string">&#x27;cumulative distribution function&#x27;</span>,xlim=<span class="title function_">c</span>(<span class="number">0</span>,<span class="number">1</span>),ylim=<span class="title function_">c</span>(<span class="number">0</span>,<span class="number">1</span>),ylab=<span class="string">&#x27;PDF&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">0.5</span>,<span class="number">0.5</span>),col=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">1</span>,<span class="number">2</span>),col=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">2</span>,<span class="number">2</span>),col=<span class="string">&#x27;pink&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">2</span>,<span class="number">5</span>),col=<span class="string">&#x27;orange&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">1</span>,<span class="number">3</span>),col=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"><span class="title function_">lines</span>(x,<span class="title function_">pbeta</span>(x,<span class="number">5</span>,<span class="number">1</span>),col=<span class="string">&#x27;black&#x27;</span>)</span><br><span class="line"><span class="title function_">legend</span>(<span class="string">&#x27;topleft&#x27;</span>,legend=<span class="title function_">c</span>(<span class="string">&#x27;α=0.5,β=0.5&#x27;</span>,<span class="string">&#x27;α=1,β=2&#x27;</span>,<span class="string">&#x27;α=2,β=2&#x27;</span>,<span class="string">&#x27;α=2,β=5&#x27;</span>,<span class="string">&#x27;α=1,β=3&#x27;</span>,<span class="string">&#x27;α=5,β=1&#x27;</span>),col=<span class="title function_">c</span>(<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;green&#x27;</span>,<span class="string">&#x27;pink&#x27;</span>,<span class="string">&#x27;orange&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>,<span class="string">&#x27;black&#x27;</span>),lwd=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC9FDF0DC006B4181B798A0E49318933F.jpeg"></p>
<p>从Beta分布的概率密度函数的图形我们可以看出，Beta分布有很多种形状，但都是在 0-1 区间内，因此Beta分布可以描述各种 0-1 区间内的形状（事件）。因此，它特别适合为某件事发生或者成功的概率建模。同时，当 α&#x3D;1，β&#x3D;1 的时候，它就是一个均匀分布。</p>
<p>下面我们使用三个例子来描述Beta分布的应用。</p>
<p>二、为实验成功概率建模（为棒球运动员的击球率建模）</p>
<p>Statlect网站上给出了一个简单的解释。假设一个概率实验只有两种结果，一个是成功，概率是X，另一个是失败，概率为(1−X)。其中，X 的值我们是不知道的，但是它所有可能的情况也是等概率的。如果我们对X的不确定性用一种方式描述，那么，可以认为X是一个来自于[0,1]区间的均匀分布的样本。这是很合理的，因为X只可能是[0,1]之间的某个值。同时，我们对X也一无所知，认为它是[0,1]之间任何一个可能的值。这些都与[0,1]均匀分布的性质契合。现在，假设我们做了n次独立重复的实验，我们观察到k次成功，n−k次失败。这时候我们就可以使用这些实验结果来修订之前的假设了。换句话说，我们就要计算X的条件概率，其条件是我们观察到的成功次数和失败次数。这里计算的结果就是Beta分布了。在这里，在总共n次实验，k次成功的条件下，X的条件概率是一个Beta分布，其参数是k+1和n−k+1。</p>
<p>在Cross Validated的问题：What is the intuition behind beta distribution?中，David Robinson给出了另外一个关于击中棒球的例子。在棒球运动中，有个叫平均击球率的概念。就是用一个运动员击中棒球的次数除以他总的击球数量。一般情况下，棒球运动员的击球概率在0.266左右。高于这个值就是不错的运动员了。假设我们要预测一个运动员在某个赛季的击球率，我们可以使用已有的数据计算。但是在赛季刚开始的时候，他击球次数少，因此无法准确预测。比如他只打了一次球，那击球率就是1或者0，这个显然是不对的，我们也不会这么预测。因为我们都有一个先验期望。即根据历史情况，我们认为一个运动员大概的击球率应当是在0.215到0.360之间。因此，当一个运动员在赛季开始就被三振出局，那么我们可以预期这个运动员的击球率可能会略低于平均值，但他不可能是0。那么，在这个运动员的例子中，关于在赛季开始的击球情况，可以使用二项式分布表示，也就是一系列击球成功和失败的实验（假设之间相互独立）。同时，我们也会给这个数据一个先验期望（即统计中的先验知识），这个先验的分布一般就是Beta分布。这里的Beta分布就是用来修正我们观测到的运动员的击球率的（简单来说就是即便开始这个运动员被三振出局了，我们也只会预测他的击球率可能低于平均水平，但不会是0）。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F4C1B8755BBF04DDBB4DCAFB49940F2F0.jpeg" alt="图1 Beta分布作为先验"></p>
<p>如上图1所示，我们使用Beta分布作为先验来解决这个问题。这个图是这个问题的概率图模型，假设该用户的击球率的分布是一个参数为θ的分布（这里θ既表示一个分布，也是这个分布的参数。因为在概率图模型中，我们经常使用某个分布的参数来代替说明某个模型），也就是说θ是用户击球成功的概率。假设，到目前为止，用户在这个赛季总共打了n次球，击中的次数是x，这是一个二项式分布，即 p(y∣θ)&#x3D;Binomial(x;n,θ)。我们的目标就是推导θ分布的形式并估算这个参数的值。这就变成了在贝叶斯推断中的求后验概率的问题了：p(θ∣y,α,β)&#x3D;​p(y)​​p(y∣θ)p(θ∣α,β)​​</p>
<p>在这里，分母 p(y) 是数据结果，也就是常数。分子第一个项是二项式分布，即 p(y∣θ) &#x3D; θ​x​​(1−θ)​(n−x)​​，分子的第二项是Beta分布的结果了。详细结果后面再说。在这里，最后我们会发现 θ 也是一个Beta分布。其结果为 Beta(α+x, β+(n−x))</p>
<p>比如，假设所有的运动员击球率在0.27左右，范围一般是0.21到0.35之间。这个可以用参数α&#x3D;81和β&#x3D;219的Beta分布表示，即 Beta(81,219)。为什么参数取这两个值呢？因为这两个参数的Beta分布的均值是 0.27，主要的区间是[0.2,0.35]。假设某个用户击球 300次，成功 100次，那么，根据计算的结果，用户的击球率的分布应当是 Beta(181,419)，其概率大约是均值 0.303，要比平均水平略高。</p>
<p>从上面这两个例子中我们可以看出，对于某个事件发生的可能的概率，当我们只有一些大概的了解，但无法知道确切的概率的时候，可以使用Beta分布表示这个概率分布。也就是说，Beta分布是用来为某些具有一定范围的事情建模的，例如 0-1 之间的概率。</p>
<p>三、为顺序统计量建模</p>
<p>假设有个机器可以随机产生[0, 1]之间的随机数，机器运行10次，第7大的数是什么，偏离不超过0.01？这个问题的数学化表达如下：</p>
<p>　　1.	X​1​​, X​2​​, …, X​n​​∼Uniform(0, 1), i.i.d.</p>
<p>　　2.	将这 n 个随机变量排序得到顺序统计量 X​(1)​​, X​(2)​​, …, X​(n)​​</p>
<p>　　3.	问 X​(k)​​ 的分布是什么？</p>
<p>　　我们可以假设计算 X​k​​ 落在 [x,x+Δx] 区间上的概率：P(x≤X​k​​≤x+Δx)&#x3D;?</p>
<p>　　我们将区间分成三个部分 [0,x), [x,x+Δx], (x+Δx,1]。假设只有1个数落在区间 [x,x+Δx] 内，那么该事件可以表示：E&#x3D;{X​1​​∈[x,x+Δx], X​i​​∈[0,x)], X​j​​∈(x+Δx,1]}</p>
<p>　　其中，i&#x3D;2 ,…, k,   j&#x3D;k+1, …, n</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F484C7843BD55414EB95325307A86C5E3.jpeg"></p>
<p>　　从而有：P(E)&#x3D;∏​i&#x3D;1​n​​P(x​i​​)&#x3D;x​k−1​​(1−x−Δx)​n−k​​Δx&#x3D;x​k−1​​(1−x)​n−k​​Δx+o(Δx)</p>
<p>　　其中o(Δx)表示Δx的高阶无穷小。根据推断，落在[x,x+Δx]区间的事件超过一个，则对应的事件概率就是o(Δx)。进而我们可以得到X​k​​的概率密度为：</p>
<p>f(x)&#x3D;lim​Δx→0​​​Δx​​P(x≤X​k​​≤x+Δx)​​</p>
<p>&#x3D;​(k−1)!(n−k)!​​n!​​x​k−1​​(1−x)​n−k​​</p>
<p>&#x3D;​Γ(k)Γ(n−k+1)​​Γ(n+1)​​x​k−1​​(1−x)​n−k​​</p>
<p>&#x3D;​Γ(α)Γ(β)​​Γ(α+β)​​x​α−1​​(1−x)​β−1​​</p>
<p>　　上式即为一般意义上的beta分布。具体的推导过程可以参见《LDA数学八卦》。</p>
<p>四、旧货商服务质量推断</p>
<p>假设亚马逊上有三家旧货商，其评价结果分别如下：</p>
<p>商家一：85193个评论，94%的正向</p>
<p>商家二：20785个评论，98%的正向</p>
<pre><code>商家三：840个评论，99%的正向
</code></pre>
<p>那么这三个商家中，哪一家的服务质量最好呢？假设这三家的服务质量分别是θ​X​​、θ​Y​​和θ​Z​​。假设我们对三家旧货商的信息一无所知，那么这些参数的先验可以认为是一个均匀分布，也可以等同于beta(1,1)。根据之前的知识，我们知道，最终这三家旧货商的服务质量应当服从三个不同参数的Beta分布，即beta(80082,5113)、beta(20370,417)和beta(833,9)（把正向的和负向的评论书算出来，分别加1就是参数了，参考上面公式）。注意，当Beta分布的参数很大的时候，我们可以使用相同均值和方差的正态分布代替这个beta分布。因此，最终这三家供货商，商家3的服务质量的标准差是0.003，是最大的。其他两家的标准差比这个还小。因此，我们可以认为这三家供货商的服务质量都高度聚焦于他们的均值。因此，从第一个或第二个分布中抽取的样本不太可能比第三个样本的值高。也就是说前两个服务商不太可能质量比第三个高。</p>
<p>参考1：<a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution">https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution</a></p>
<p>参考2：<a target="_blank" rel="noopener" href="https://www.johndcook.com/blog/2011/09/27/bayesian-amazon/#comments">https://www.johndcook.com/blog/2011/09/27/bayesian-amazon/#comments</a></p>
<p>参考3：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Beta_distribution">https://en.wikipedia.org/wiki/Beta_distribution</a></p>
<p>参考4：《LDA数学八卦》</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F(Exponential%20Family)%E7%9B%B8%E5%85%B3%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%8F%8A%E5%9C%A8%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%20%20%20%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%80%85%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99(Datalearner)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E6%8C%87%E6%95%B0%E5%88%86%E5%B8%83%E6%97%8F(Exponential%20Family)%E7%9B%B8%E5%85%B3%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC%E5%8F%8A%E5%9C%A8%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%20%20%20%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%80%85%E5%AE%98%E6%96%B9%E7%BD%91%E7%AB%99(Datalearner)/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:23" itemprop="dateModified" datetime="2022-11-07T06:06:23+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本博客的CSDN地址为：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qy20115549/article/details/87247363">https://blog.csdn.net/qy20115549/article/details/87247363</a></p>
<p>指数分布族的概念</p>
<p>指数分布族是一系列分布的统称，包含连续和离散的相关分布。例如，正态分布(Gaussian)、泊松分布（Poisson）、二项分布(Bernoulli)、指数分布(exponential)、Gamma分布、多项式分布(multivariate)等。</p>
<p>指数分布族中的分布以及指数分布族的性质，经常用于机器学习(machine learning)模型的参数假设以及参数推理中。较为典型的模型是生成模型，例如主题模型(Topic Models)中经常使用到的共轭分布(multivariate和Dirichlet分布、Bernoulli和Beta分布、Poisson和gamma分布等)。指数分布族中的共轭经常用于参数推理、另外其统计特性经常用于变分推理。例如，有兴趣的可以详细阅读下面几篇文章：</p>
<ul>
<li><p>Blei D M, Ng A Y, Jordan M I. Latent dirichlet allocation[J]. Journal of machine Learning research, 2003, 3(Jan): 993-1022.</p>
</li>
<li><p>Teh Y W, Newman D, Welling M. A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation[C]&#x2F;&#x2F;Advances in neural information processing systems. 2007: 1353-1360.</p>
</li>
<li><p>Blei D M, Kucukelbir A, McAuliffe J D. Variational inference: A review for statisticians[J]. Journal of the American Statistical Association, 2017, 112(518): 859-877. 【变分推断的综述性文章—案例代码为：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qy20115549/article/details/86694325%E3%80%91">https://blog.csdn.net/qy20115549/article/details/86694325】</a></p>
</li>
<li><p>Su J. Variational Inference: A Unified Framework of Generative Models and Some Revelations[J]. arXiv preprint arXiv:1807.05936, 2018. 【变分自编码器VAE、生成对抗网络GAN】</p>
</li>
<li><p>Wainwright M J, Jordan M I. Graphical models, exponential families, and variational inference[J]. Foundations and Trends® in Machine Learning, 2008, 1(1–2): 1-305. 【一本书】</p>
</li>
</ul>
<p>指数分布族中的分布于都写成下面的形式：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F355AE048E7574E7A93D06E5C0DD917F7.png"></p>
<p>其中：</p>
<ul>
<li><p>$\eta$为自然参数(natural parameter)，可以是向量形式</p>
</li>
<li><p>$T(x)$为充分统计量(sufficient statistic)</p>
</li>
<li><p>$A(\eta)$为累计函数(cumulant function)，作用是确保概率和为1</p>
</li>
<li><p>$h(x)$为underlying measure</p>
</li>
</ul>
<p>典型分布转化</p>
<p>Bernoulli分布</p>
<p>以下是Bernoulli分布的转化：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD830BAE0D9494DB8B2C5B3A6FA4EE766.png"></p>
<p>对比上面的形式，可以得到：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCD511D07FC324402AF19A65CA9F57558.png"></p>
<p>Poisson分布</p>
<p>泊松分布的标准形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F53AE61F2041947B4B143A55B7C7CF93B.png"></p>
<p>写成指数形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FEC21B8BDD0B14E32ADC1DAEFDE07598D.png"></p>
<p>因此泊松分布也属于指数分布族，其相关参数为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA2684B7E4BF4470C9C4D24447F17997A.png"></p>
<p>Gaussian分布</p>
<p>正太分布的形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F89A56357778F4362BB81FC9899DD9362.png"></p>
<p>写成指数形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA3A473BD4E9F4D8FA83292D2E948D0A3.png"></p>
<p>因此，也满足指数组分布：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FAC00F737244C4E7C9AB1F4F64E1F0016.png"></p>
<p>高斯分布有两个参数，因此自然参数以及充分统计量都有两个。</p>
<p>多元Gaussian分布</p>
<p>标准形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FE4739622C35E4B4DB2FA3B558F884D43.png"></p>
<p>写成指数族形式：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7437ED5F89384BF8A7F2DE1CD54C7A9D.png"></p>
<p>对比：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F61735DDACAA147309C6F24A41B14CD4A.png"></p>
<p>可以得到：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDB67DF8FA5C94659A20CF3A9EE86C1B3.png"></p>
<p>自然参数为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5C770785BF504CD5B25A1AD0F587D3BA.png"></p>
<p>cumulant function为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1DE1A19E849E40D3964DFF0D0BAFE68F.png"></p>
<p>Multinomial分布</p>
<p>多项式分布的形式为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6C23542FB769462F9F0678E1E86E2249.png"></p>
<p>重写为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FAE67AF7306844F058005FFB795420308.png"></p>
<p>从这里发现，累计函数A(η)为0了，实际上并不为0。继续转化有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F983499B104274DBFA6584E4E1D4C14B9.png"></p>
<p>这里有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F78BA56BF7B214946949D3CCEA02B2D6C.png"></p>
<p>因此，可以得到：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1683F89449414320BD4A1F0FA9F024B6.png"></p>
<p>由这个式子可以转化得到π​k​​，即：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA713095596FA43EB81B423F6615185D2.png"></p>
<p>可以看出这个式子是softmax函数。</p>
<p>另外，我们也可以获得：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6D3E1AAAE68B4352AC17FD164A59ADE8.png"></p>
<p>变分推断应用</p>
<p>在变分推理中，经常使用到的是A(η)性质，即A(η)对(η的一阶偏导数：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F139CE39B1BFF4DFFA418E329445A5190.png"></p>
<p>上面这个公式，可以由最原始的公式得到。继续计算有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB77A1B2A4B8744A788077956C0CFD4F4.png"></p>
<p>例如，对二项分布而言：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD3992DB505B54A4E8A8693775EA6AE5C.png"></p>
<p>对正太分布而言：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6F65A0DAA52E462EB46418D24CEEAD0F.png"></p>
<p>在变分推理中，经常要计算期望，通过这个性质，便可以将期望计算转化成求导计算。例如，</p>
<p>LDA模型</p>
<p>LDA的概率图表示如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F0368810892DF48AB8FBB9EE7B0E2B05B.png"></p>
<p>主题分布θ服从先验为α的Dirichlet分布，即：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9B145D868C6E4E9984B51B1C86C9AD8A.png"></p>
<p>其中：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2E719DF0CC4B46D38E95D3D92A2AA270.png"></p>
<p>对θ的分布进行转化有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FADFCD2D7127146A88B915EC4559C3357.png"></p>
<p>因此，可以看出Dirichlet分布也属指数分布，由上面的公式得到：</p>
<p>自然参数η​i​​:</p>
<p>sufficient statistic为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD569E4D9B352436C99E410440DE8A263.png"></p>
<p>log normalizer或cumulant function为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FBAA512B2365A450FAADF0A6A16D42F2B.png"></p>
<p>基于上面这三个公式有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDD3FF039D805482C9A065C2E0BA84273.png"></p>
<p>在LDA的变分推理中，需要将下界ELOB转化为多项期望，如下面所示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF5378981CE134DB9947D9D1446BC979C.png"></p>
<p>此公式中，包含多个期望，在计算时，每个期望都需要推导出公式。由于前面已经分析参数θ，下面只例举E​q​​[logp(θ​j​​∣α)]:</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5D471334BE3D47E3B6534B6F5E8DAC75.png"></p>
<p>在上面公式标红的部分，便可转化成偏导的计算，这里θ对应的变分参数为γ，即：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC3E0D2B11D0C4F98942A7EC58BED2F01.png"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF06CF89D7B9642409634CE5AAA294710.png"></p>
<p>这里的log normalizer或cumulant function为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2A2CC54F0D4446CD9B46E282CD1EB55C.png"></p>
<p>进而可以计算公式标红的期望：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1C014331DEFC48A1BC70DB61FD8DCC33.png"></p>
<p>其中，Ψ(⋅)为digamma函数，及Gamma函数对数的一阶偏导数。因此有：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FED97BB060F6646F7AE3D60BE2238CD8A.png"></p>
<p>关于其他期望的求法与这个类似，这里不作过多赘述，有兴趣的可以学习这篇文章：</p>
<p>Inference Methods for Latent Dirichlet Allocation</p>
<p>参考内容</p>
<p><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf">http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter9.pdf</a></p>
<p><a target="_blank" rel="noopener" href="http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf">http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf</a> [lda推理]</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E9%AB%98%E9%98%B6%E6%97%A0%E7%A9%B7%E5%B0%8F%20%E5%88%99%E6%98%AF%E6%9C%89%E7%95%8C%E9%87%8F%E8%80%8C%E4%B8%8D%E6%98%AF%E5%90%8C%E9%98%B6%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E9%AB%98%E9%98%B6%E6%97%A0%E7%A9%B7%E5%B0%8F%20%E5%88%99%E6%98%AF%E6%9C%89%E7%95%8C%E9%87%8F%E8%80%8C%E4%B8%8D%E6%98%AF%E5%90%8C%E9%98%B6%E9%87%8F/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 01:58:06" itemprop="dateModified" datetime="2022-11-07T01:58:06+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>小o是高阶无穷小,大O则是有界量而不是同阶量</p>
<p>具体一点讲,如果给定某个变化趋势x-&gt;a,</p>
<p>1.若lim f(x)&#x2F;g(x)&#x3D;0,那么记f(x) &#x3D; o(g(x))；</p>
<p>2.若存在M&gt;0使得|f(x)&#x2F;g(x)|0,那么记f(x)&#x3D;Ω(g(x))</p>
<p>4.若00时o(x^k)可以化到O(x^{k+1}),比如k+1阶可微函数的n阶Maclaurin展开的余项就有这两种形式.不过一般是不成立的,比如x-&gt;0时x&#x2F;lnx&#x3D;o(x),但是不能化到o(x^2).</p>
<p>追答：谁告诉你|f(x)&#x2F;g(x)|≦M就是只相差常数倍，那样还要大Θ记号干什么，有没有下界区别很本质。另外我也写过o(u)可以写成O(u)的结论。既然我写了那么多你就应该仔细看几遍再仔细想想。</p>
<p> 大O是有界量，f(x)&#x3D;O(g(x))只表明“f可以被g控制”，不说明它们同阶，比如说 x→0时x^3&#x3D;O(x) n→+∞时lnn&#x3D;O(n) 这些都不是同阶量。</p>
<p>当limA&#x3D;0时：</p>
<p>如果limB&#x2F;A&#x3D;0，就说B是比A高阶的无穷小，记作B&#x3D;o(A)，表示B比A趋近于零的速度更快；</p>
<p>如果limB&#x2F;A&#x3D;∞，就说B是比A低阶的无穷小</p>
<p>如果limB&#x2F;A&#x3D;c≠0，就说B与A是同阶无穷小</p>
<p>如果limB&#x2F;A&#x3D;1，就说B与A是等价无穷小，记作B~A；</p>
<p>如果limB&#x2F;(A^k)&#x3D;c≠0，就说B是关于A的k价无穷小；</p>
<p>定义</p>
<pre><code>    O(x):若对于任意的x,存在常数k,使得f(x)&lt;=k*x，那么f(x)是属于O(x)的；       

    同理，若对于任意的x,存在常数k，使得g(x)&lt;=k*f(x)，那么g(x)是属于O[f(x)]的。
</code></pre>
<p>解释</p>
<pre><code>    即O[f(x)]是g(x)的上界的常数倍，为了表征g(x)的性质，通常取其上确界约化系数后的形式。
</code></pre>
<p>举例</p>
<pre><code>     1）f(x)＝x^2+x+1是O(x^2)的（当然也是O(x^3)的，但是为了更准备地表明f(x)的性质，通常我们取O(x^2))

     2）f(x)＝sin(x)是O(1)的（当然也是O(x)的，想怎么用都行，看具体条件）

     3)　f(n)＝n!是O(n^n)的（这个更明显，看你想怎么用吧）
</code></pre>
<p>是用于描述函数渐进行为的数学符号。更确切地说，它是用另一个（通常更简单的）函数来描述一个函数数量级的渐近上界。在数学中，它一般用来刻画被截断的无穷级数尤其是渐近级数的剩余项；在计算机科学中，它在分析算法复杂性的方面非常有用。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%BC%BD%E9%A9%AC%E5%88%86%E5%B8%83%E4%B8%8E%20%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83%20-%20%E5%86%B7%E6%9C%88%E6%97%A0%E5%A3%B0%E7%9A%84%E5%8D%9A%E5%AE%A2%20-%20CSDN%E5%8D%9A%E5%AE%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%BC%BD%E9%A9%AC%E5%88%86%E5%B8%83%E4%B8%8E%20%E8%B4%9D%E5%A1%94%E5%88%86%E5%B8%83%20-%20%E5%86%B7%E6%9C%88%E6%97%A0%E5%A3%B0%E7%9A%84%E5%8D%9A%E5%AE%A2%20-%20CSDN%E5%8D%9A%E5%AE%A2/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:58" itemprop="dateModified" datetime="2022-11-07T06:06:58+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29831163/article/details/89379731">https://blog.csdn.net/qq_29831163/article/details/89379731</a></p>
<p>伽马分布 </p>
<p>伽马函数 ：称  </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB714957EF0054409971E990B69F579AC.gif"></p>
<p>  为伽马函数，其中参数 α&gt;0 ，伽马函数具有如下性质：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F3CDE47F2DB4D4B9AA73396B3A6812BA5.gif"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF3A23D9FDD2041209A50E84791431147.gif"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5C684AA18E4746ED9ED8513C39D55B3A.gif"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F048F3FD2BAA84513AD381653C0C63B63.gif"></p>
<ul>
<li>其中 n 为自然数；或写作</li>
</ul>
<p> </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F79659C6D15AD478A9EB369D6B768FDA1.gif"></p>
<p>余元公式：对于x∈（0,1），有 </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2A27A28281AE4FE794DCFA5FE6FFB158.gif"></p>
<ul>
<li>与贝塔函数 B(m,n) 的关系 :</li>
</ul>
<p> </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5717DD34065744F39B365802D5FD1F57.gif"></p>
<ul>
<li><p>对于 x&gt;0 ;伽马函数是严格凹函数。</p>
</li>
<li><p>x足够大时，可以用Stirling 公式来计算Gamma 函数值:</p>
</li>
</ul>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD07E6FF3C3D84C7FB586AD1FC6990B2D.gif"></p>
<p>背景：若一个元器件能抵挡一些外来冲击，但遇到第k次冲击即告失效，则第k 次冲击来到的时间X(寿命)服从形状参数为k的伽马分布 。</p>
<p>密度函数:</p>
<p>  α&gt;0 为形状参数 ， λ&gt;0 为尺度参数 ；</p>
<p>密度函数图如下所示，</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2D6CB9F3842E4353B79A8612E4B5EE14.png"></p>
<p>数学期望与方差</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6F61DE1C6E614253AB4605ED3E08EEDC.gif"></p>
<p>与指数分布 的关系</p>
<p>若形状参数为整数k,则伽马变量可以表示成k个独立同分布的指数变量之和。即，</p>
<p>若 ,则</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F599635A23B5646DD8A1905B64E5EADB4.gif"></p>
<p> ,其中 </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6D90063119284ED9A2F1D1998FEC6C4F.gif"></p>
<p> 【独立同分布】</p>
<p>卡方分布 </p>
<p>与伽马分布的关系</p>
<p> 称 的伽马分布为自由度为n的卡方分布，即</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC4AF557F339E4968A7470B53638AC87A.gif"></p>
<p>密度函数</p>
<p>期望与方差</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCA9CCCB0FA02478487299E066FA78AE0.gif"></p>
<p>注：后期再讲数理统计中的t分布与F分布时，再重新细讲卡方分布。参考重要抽样分布：卡方分布（χ2分布）、t分布和F分布</p>
<p>贝塔分布 </p>
<p>背景：很多比率，比如，产品的不合格率、机器的维修率、某商品的市场占有率、射击的命中率….都是在区间（0，1）上取值的随机变量，可用beta分布来描述这些随机变量</p>
<p>贝塔函数 ：称  </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F4E4F2530D12A4601B680F378D391D57D.gif"></p>
<p> 为贝塔函数，其中参数 a&gt;0，b&gt;0 。贝塔函数的性质：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F93F6AC1CC566444389EDCE3DD89917C8.gif"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F87EF12E824F549E99AA0243513CA430D.gif"></p>
<p>密度函数 ：当 0&lt;x&lt;1 时，为 f(x)；否则为 0。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F890E5027B6454B4FBCA8D16D328D08A1.png"></p>
<p>其中 α &gt;0, β&gt;0 都是形状参数。【下图中 a就是α ，b就是 β】</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDE0F9FB745CE4069B204AA251D3B0CA0.png"></p>
<p> 贝塔分布是定义在（0，1）区间上的连续概率分布，是伯努利分布和二项式分布的共轭先验分布。</p>
<p>数学期望与方差：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F0F82D7566E594FE0A945E252054A1A40.gif"></p>
<p>与均匀分布的关系：</p>
<p>当  a&#x3D;b&#x3D;1 时的贝塔分布就是区间（0，1）上的均匀分布，即 </p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8C619F6F2EE74DE088D9A85B9C2681B4.gif"></p>
<p>贝塔分布（Beta Distribution) 是一个作为伯努利分布和二项式分布的共轭先验分布的密度函数，在机器学习和数理统计学中有重要应用。在概率论中，贝塔分布，也称Β分布，是指一组定义在(0,1) 区间的连续概率分布，有两个参数 α&gt;0, β&gt;0。</p>
<p>Β分布的概率密度函数是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8A84D65B3ACB4B71BA1E6A11F51215E3.png"></p>
<p>其中 Γ(z) Γ函数。随机变量X服从参数为 α, β 的Β分布，通常写作 X~ Be(α, β)。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F0D5CB61FD04F4F9E8A7B3E622BD44C7B.png"></p>
<p>图1.概率密度函数</p>
<p>Β分布的累积分布函数是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1084C2B874E4472C9AB784F57FB5F1C2.png"></p>
<p>其中  Bx(α, β ) 是不完全Β函数；Ix(α, β ) 是正则不完全贝塔函数。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F92F2019FFBFF45ED96CD4985BC25129E.png"></p>
<p>图2.累积分布函数</p>
<p>性质</p>
<ol>
<li>参数为 α, β  贝塔分布的众数是：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F34094021DB284E6BA184EC0C02F44ACC.png"></p>
<p>2.期望值和方差分别是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB1374575A0364F8EA5D87BE58853464B.png"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FFB8CE983900847CAB7D812E43770CAED.png"></p>
<p>3.偏度是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F90B51CC933234FEBA260F5973521FD29.png"></p>
<p>4.峰度是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCDE530A22C9E4CEF96A4C43D8A3AAB44.png"></p>
<p>或：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F37164A5767894BC7B86AAEBF83D4FFF3.png"></p>
<ol start="5">
<li>k 阶矩是：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB280547242034848B27CAECD3FC37537.png"></p>
<p>其中 (x)k 表示下降阶乘幂。 k 阶矩还可以递归地表示为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FFBCA076C92EF416C8018934E5FF5F995.png"></p>
<p>6.满足下面式子：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F82ADBAE9A73A4C38927BC578B9D4C118.png"></p>
<ol start="7">
<li>给定两个Β分布随机变量, X~ Beta(α, β)，Y~ Beta(α’, β’), X 的微分熵为：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F622A1E1F6A1C4737B667413F77242269.png"></p>
<p>其中 Ψ 表示双伽玛函数。</p>
<ol start="8">
<li>联合熵为：</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9300DA4591B342E5A3A65EAF61965FD4.png"></p>
<p>9.KL散度其为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCB5D50042C934AB4B41D12C69A3429D4.png"></p>
<p>实例</p>
<p>空气中含有的气体状态的水分。表示这种水分的一种办法就是相对湿度。即现在的含水量与空气的最大含水量（饱和含水量）的比值。我们听到的天气预告用语中就经常使用相对湿度这个名词。</p>
<p>相对湿度的值显然仅能出现于0到1之间（经常用百分比表示）。而空气为什么出现某个相对湿度显然具有随机性（可以利用最复杂原理），这些提示我们空气的相对湿度可能符合贝塔分布。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%B8%BB%E6%88%90%E4%BB%BD%E5%88%86%E6%9E%90(PCA)%E6%9C%80%E8%AF%A6%E7%BB%86%E5%92%8C%E5%85%A8%E9%9D%A2%E7%9A%84%E8%AF%A0%E9%87%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%B8%BB%E6%88%90%E4%BB%BD%E5%88%86%E6%9E%90(PCA)%E6%9C%80%E8%AF%A6%E7%BB%86%E5%92%8C%E5%85%A8%E9%9D%A2%E7%9A%84%E8%AF%A0%E9%87%8A/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:19" itemprop="dateModified" datetime="2022-11-07T06:06:19+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>主成分分析（Principal components analysis）</p>
<p>-最大方差解释</p>
<ol>
<li>问题</li>
</ol>
<p>     真实的训练数据总是存在各种各样的问题：</p>
<p>1、 比如拿到一个汽车的样本，里面既有以“千米&#x2F;每小时”度量的最大速度特征，也有“英里&#x2F;小时”的最大速度特征，显然这两个特征有一个多余。</p>
<p>2、 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？</p>
<p>3、 拿到一个样本，特征非常多，而样例特别少，这样用回归去直接拟合非常困难，容易过度拟合。比如北京的房价：假设房子的特征是（大小、位置、朝向、是否学区房、建造年代、是否二手、层数、所在层数），搞了这么多特征，结果只有不到十个房子的样例。要拟合房子特征-&gt;房价的这么多特征，就会造成过度拟合。</p>
<p>4、 这个与第二个有点类似，假设在IR中我们建立的文档-词项矩阵中，有两个词项为“learn”和“study”，在传统的向量空间模型中，认为两者独立。然而从语义的角度来讲，两者是相似的，而且两者出现频率也类似，是不是可以合成为一个特征呢？</p>
<p>5、 在信号传输过程中，由于信道不是理想的，信道另一端收到的信号会有噪音扰动，那么怎么滤去这些噪音呢？</p>
<p>     回顾我们之前介绍的《模型选择和规则化》，里面谈到的特征选择的问题。但在那篇中要剔除的特征主要是和类标签无关的特征。比如“学生的名字”就和他的“成绩”无关，使用的是互信息的方法。</p>
<p>     而这里的特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。</p>
<p>     下面探讨一种称作主成分分析（PCA）的方法来解决部分上述问题。PCA的思想是将n维特征映射到k维上（k</p>
<ol start="2">
<li>PCA计算过程</li>
</ol>
<p>     首先介绍PCA的计算过程：</p>
<p>     假设我们得到的2维数据如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF9CAF47D0EC44356A2DF4A8038314D27.webp"></p>
<p>     行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。也可以认为有10辆汽车，x是千米&#x2F;小时的速度，y是英里&#x2F;小时的速度，等等。</p>
<p>第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB9CF57A03E9345CB85CAF6A1A228F0AD.webp"></p>
<p>第二步，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB0FC309D3FFD40F598E64BEC7A90CCDF.webp"></p>
<p>     这里只有x和y，求解得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F350E41B7F5FF4C0B924CD6D133AB7413.webp"></p>
<p>     对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。</p>
<p>第三步，求协方差的特征值和特征向量，得到</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8781F316BC154814AE4FF330EADE0131.webp"></p>
<p>     上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UGE9l9r0r79zA7Qaia619gN7nkllr7WaJ7NfYOSpXyGOR5wyrZDVCowPibccGCWibuWvbbFjPQPrx3DOOW1jtmRMQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1"></p>
<p>，这里的特征向量都归一化为单位向量。</p>
<p>第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。</p>
<p>     这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UGE9l9r0r79zA7Qaia619gN7nkllr7WaJNNhfdaVH1XsSjZSaRegugXDyhMaIMicibcpUweP8wTsRZiaUpNGbqcWHA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1"></p>
<p>。</p>
<p>第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m<em>n)，协方差矩阵是n</em>n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UGE9l9r0r79zA7Qaia619gN7nkllr7WaJMtSdfvDEFneA3r8vF7iaVjjdrMph3uM9oxenFl77GlOW3gthWzFrDqQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1"></p>
<p>     这里是</p>
<p>     FinalData(10<em>1) &#x3D; DataAdjust(10</em>2矩阵)×特征向量</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UGE9l9r0r79zA7Qaia619gN7nkllr7WaJNNhfdaVH1XsSjZSaRegugXDyhMaIMicibcpUweP8wTsRZiaUpNGbqcWHA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1"></p>
<p>     得到结果是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F4904AA2E8FCB4FD6A2938381205E215B.webp"></p>
<p>     这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。</p>
<p>     上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。</p>
<p>     上述过程有个图描述：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F41C42EBAC8B74E1E8F749FB90FB55596.webp"></p>
<p>     正号表示预处理后的样本点，斜着的两条线就分别是正交的特征向量（由于协方差矩阵是对称的，因此其特征向量正交），最后一步的矩阵乘法就是将原始样本点分别往特征向量对应的轴上做投影。</p>
<p>     如果取的k&#x3D;2，那么结果是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F05090AB3BC0944DF8CB3392058B052A7.webp"></p>
<p>     这就是经过PCA处理后的样本数据，水平轴（上面举例为LS特征）基本上可以代表全部样本点。整个过程看起来就像将坐标系做了旋转，当然二维可以图形化表示，高维就不行了。上面的如果k&#x3D;1，那么只会留下这里的水平轴，轴上是所有点在该轴的投影。</p>
<p>     这样PCA的过程基本结束。在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差，然后对每个样例在该特征下的数据除以。</p>
<p>     归纳一下，使用我们之前熟悉的表示方法，在求协方差之前的步骤是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9B93B3AF488A4185AFBB1E64A6A4F855.webp"></p>
<p>     其中是样例，共m个，每个样例n个特征，也就是说是n维向量。是第i个样例的第j个特征。是样例均值。是第j个特征的标准差。</p>
<p>     整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的k维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？</p>
<ol start="3">
<li>PCA理论基础</li>
</ol>
<p>     要解释为什么协方差矩阵的特征向量就是k维理想特征，我看到的有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。</p>
<p>3.1 最大方差理论</p>
<p>     在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在横轴上的投影方差较大，在纵轴上的投影方差较小，那么认为纵轴上的投影是由噪声引起的。</p>
<p>因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。</p>
<p>     比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9C2E89C348E64EF789C075FE34059747.webp"></p>
<p>     下面将样本投影到某一维上，这里用一条过原点的直线表示（前处理的过程实质是将原点移到样本点的中心点）。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8D83611A215A4AF2BFA0499E2602559B.webp"></p>
<p>     假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。</p>
<p>     这里先解释一下投影的概念：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FEDA6FBFCD71945458E38499BC1680CD1.webp"></p>
<p>     红色点表示样例，蓝色点表示在u上的投影，u是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是在u上的投影点，离原点的距离是（即或者）由于这些样本点（样例）的每一维特征均值都为0，因此投影到u上的样本点（只有一个到原点的距离值）的均值仍然是0。</p>
<p>     回到上面左右图中的左图，我们要求的是最佳的u，使得投影后的样本点方差最大。</p>
<p>     由于投影后均值为0，因此方差为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F47D78AAF4CAF4C4AA76160CDAC7DCC29.webp"></p>
<p>     中间那部分很熟悉啊，不就是样本特征的协方差矩阵么（的均值为0，一般协方差矩阵都除以m-1，这里用m）。</p>
<p>     用来表示，表示，那么上式写作</p>
<p>     由于u是单位向量，即，上式两边都左乘u得，</p>
<p><img src="http://mmbiz.qpic.cn/mmbiz_png/UGE9l9r0r79zA7Qaia619gN7nkllr7WaJw1R55Jp0pNAsFjd7JPCKkBlUFjmLtueFPbK8gM5pKAicRsT7vHW58wQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1"></p>
<p>     即</p>
<p>     We got it！就是的特征值，u是特征向量。最佳的投影直线是特征值最大时对应的特征向量，其次是第二大对应的特征向量，依次类推。</p>
<p>     因此，我们只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，样例通过以下变换可以得到新的样本。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F0221579050B941C8911087CCF4463156.webp"></p>
<p>     其中的第j维就是在上的投影。</p>
<p>     通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。</p>
<p>主成分分析（Principal components analysis）-最小平方误差解释</p>
<p>3.2 最小平方误差理论</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDB5A851A49E544159E1CFBA8895E6F46.webp"></p>
<p>     假设有这样的二维样本点（红色点），回顾我们前面探讨的是求一条直线，使得样本点投影到直线上的点的方差最大。本质是求直线，那么度量直线求的好不好，不仅仅只有方差最大化的方法。再回想我们最开始学习的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。比如这个问题中，特征是x，类标签是y。回归时最小二乘法度量的是距离d。如果使用回归方法来度量最佳直线，那么就是直接在原始样本上做回归了，跟特征选择就没什么关系了。</p>
<p>     因此，我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离d’来度量。</p>
<p>     现在有n个样本点，每个样本点为m维（这节内容中使用的符号与上面的不太一致，需要重新理解符号的意义）。将样本点在直线上的投影记为，那么我们就是要最小化</p>
<p>     这个公式称作最小平方误差（Least Squared Error）。</p>
<p>     而确定一条直线，一般只需要确定一个点，并且确定方向即可。</p>
<p>第一步确定点：</p>
<p>     假设要在空间中找一点来代表这n个样本点，“代表”这个词不是量化的，因此要量化的话，我们就是要找一个m维的点，使得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F431400C89757495799CE555C48198E04.webp"></p>
<p>     最小。其中是平方错误评价函数（squared-error criterion function），假设m为n个样本点的均值：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F95C0455529074EA383E7EFC3DFC39145.webp"></p>
<p>     那么平方错误可以写作：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F8042FFC7C87F47FAA22019F1E3B9D094.webp"></p>
<p>     后项与无关，看做常量，而，因此最小化时，</p>
<p>是样本点均值。</p>
<p>第二步确定方向：</p>
<p>     我们从拉出要求的直线（这条直线要过点m），假设直线的方向是单位向量e。那么直线上任意一点，比如就可以用点m和e来表示</p>
<p>     其中是到点m的距离。</p>
<p>     我们重新定义最小平方误差：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F28A4F46B7C36416EBF9C417D51E51D82.webp"></p>
<p>     这里的k只是相当于i。就是最小平方误差函数，其中的未知参数是和e。</p>
<p>     实际上是求的最小值。首先将上式展开：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F444DBF6643294AEC8D3912FC00157049.webp"></p>
<p>     我们首先固定e，将其看做是常量，，然后对进行求导，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FAD8BCE963B1742059C5C0806B0EEEA6B.webp"></p>
<p>     这个结果意思是说，如果知道了e，那么将与e做内积，就可以知道了在e上的投影离m的长度距离，不过这个结果不用求都知道。</p>
<p>     然后是固定，对e求偏导数，我们先将公式（8）代入，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F916C8624ADF341CB96E0EC864A651880.webp"></p>
<p>     其中</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC6CB19837C954803B86F5E16D1293A7E.webp"></p>
<p> 与协方差矩阵类似，只是缺少个分母n-1，我们称之为散列矩阵（scatter matrix）。</p>
<p>     然后可以对e求偏导数，但是e需要首先满足，引入拉格朗日乘子，来使最大（最小），令</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F80A1B3F7DB104BC185F5F7963ECD3FB9.webp"></p>
<p>     求偏导</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     这里存在对向量求导数的技巧，方法这里不多做介绍。可以去看一些关于矩阵微积分的资料，这里求导时可以将看作是，将看做是。</p>
<p>     导数等于0时，得</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     两边除以n-1就变成了，对协方差矩阵求特征值向量了。</p>
<p>     从不同的思路出发，最后得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     这时候点都聚集在新的坐标轴周围，因为我们使用的最小平方误差的意义就在此。</p>
<ol start="4">
<li>PCA理论意义</li>
</ol>
<p>     PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。再看回顾一下PCA的效果。经过PCA处理后，二维数据投影到一维上可以有以下几种情况：</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     我们认为左图好，一方面是投影后方差最大，一方面是点到直线的距离平方和最小，而且直线过样本点的中心点。为什么右边的投影效果比较差？直觉是因为坐标轴之间相关，以至于去掉一个坐标轴，就会使得坐标点无法被单独一个坐标轴确定。</p>
<p>     PCA得到的k个坐标轴实际上是k个特征向量，由于协方差矩阵对称，因此k个特征向量正交。看下面的计算过程。</p>
<p>     假设我们还是用</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>来表示样例，m个样例，n个特征。特征向量为e，表示第i个特征向量的第1维。那么原始样本特征方程可以用下面式子来表示：</p>
<p>     前面两个矩阵乘积就是协方差矩阵（除以m后），原始的样本矩阵A是第二个矩阵m*n。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     上式可以简写为</p>
<p>     我们最后得到的投影结果是，E是k个特征向量组成的矩阵，展开如下：</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     得到的新的样例矩阵就是m个样例到k个特征向量的投影，也是这k个特征向量的线性组合。e之间是正交的。从矩阵乘法中可以看出，PCA所做的变换是将原始样本点（n维），投影到k个正交的坐标系中去，丢弃其他维度的信息。举个例子，假设宇宙是n维的（霍金说是11维的），我们得到银河系中每个星星的坐标（相对于银河系中心的n维向量），然而我们想用二维坐标去逼近这些样本点，假设算出来的协方差矩阵的特征向量分别是图中的水平和竖直方向，那么我们建议以银河系中心为原点的x和y坐标轴，所有的星星都投影到x和y上，得到下面的图片。然而我们丢弃了每个星星离我们的远近距离等信息。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<ol start="5">
<li>总结与讨论</li>
</ol>
<p>     这一部分来自<a target="_blank" rel="noopener" href="http://www.cad.zju.edu.cn/home/chenlu/pca.htm">http://www.cad.zju.edu.cn/home/chenlu/pca.htm</a></p>
<p>     PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>     PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 </p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>图表 4：黑色点表示采样数据，排列成转盘的形状。 </p>
<p>     容易想象，该数据的主元是或是旋转角。</p>
<p>     如图表 4中的例子，PCA找出的主元将是。但是这显然不是最优和最简化的主元。之间存在着非线性的关系。根据先验的知识可知旋转角是最优的主元（类比极坐标）。则在这种情况下，PCA就会失效。但是，如果加入先验的知识，对数据进行某种划归，就可以将数据转化为以为线性的空间中。这类根据先验知识对数据预先进行非线性转换的方法就成为kernel-PCA，它扩展了PCA能够处理的问题的范围，又可以结合一些先验约束，是比较流行的方法。</p>
<p>     有时数据的分布并不是满足高斯分布。如图表 5所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使。这一类方法被称为独立主元分解(ICA)。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>图表 5：数据的分布并不满足高斯分布，呈明显的十字星状。 </p>
<p>     这种情况下，方差最大的方向并不是最优主元方向。</p>
<p>     另外PCA还可以用于预测矩阵中缺失的元素。</p>
<p>独立成分分析（Independent Component Analysis）</p>
<ol>
<li>问题：</li>
</ol>
<p>     1、上节提到的PCA是一种数据降维的方法，但是只对符合高斯分布的样本点比较有效，那么对于其他分布的样本，有没有主元分解的方法呢？</p>
<p>     2、经典的鸡尾酒宴会问题（cocktail party problem）。假设在party中有n个人，他们可以同时说话，我们也在房间中一些角落里共放置了n个声音接收器（Microphone）用来记录声音。宴会过后，我们从n个麦克风中得到了一组数据</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>，i表示采样的时间顺序，也就是说共得到了m组采样，每一组采样都是n维的。我们的目标是单单从这m组采样数据中分辨出每个人说话的信号。</p>
<p>     将第二个问题细化一下，有n个信号源，，每一维都是一个人的声音信号，每个人发出的声音信号独立。A是一个未知的混合矩阵（mixing matrix），用来组合叠加信号s，那么</p>
<p>     x的意义在上文解释过，这里的x不是一个向量，是一个矩阵。其中每个列向量是，</p>
<p>     表示成图就是</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     这张图来自</p>
<p><a target="_blank" rel="noopener" href="http://amouraux.webnode.com/research-interests/research-interests-erp-analysis/blind-source-separation-bss-of-erps-using-independent-component-analysis-ica/">http://amouraux.webnode.com/research-interests/research-interests-erp-analysis/blind-source-separation-bss-of-erps-using-independent-component-analysis-ica/</a></p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>的每个分量都由的分量线性表示。A和s都是未知的，x是已知的，我们要想办法根据x来推出s。这个过程也称作为盲信号分离。</p>
<p>     令，那么</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     将W表示成</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     其中，其实就是将写成行向量形式。那么得到：</p>
<ol start="2">
<li>ICA的不确定性（ICA ambiguities）</li>
</ol>
<p>     由于w和s都不确定，那么在没有先验知识的情况下，无法同时确定这两个相关参数。比如上面的公式s&#x3D;wx。当w扩大两倍时，s只需要同时扩大两倍即可，等式仍然满足，因此无法得到唯一的s。同时如果将人的编号打乱，变成另外一个顺序，如上图的蓝色节点的编号变为3,2,1，那么只需要调换A的列向量顺序即可，因此也无法单独确定s。这两种情况称为原信号不确定。</p>
<p>     还有一种ICA不适用的情况，那就是信号不能是高斯分布的。假设只有两个人发出的声音信号符合多值正态分布，，I是2*2的单位矩阵，s的概率密度函数就不用说了吧，以均值0为中心，投影面是椭圆的山峰状（参见多值高斯分布）。因为，因此，x也是高斯分布的，均值为0，协方差为</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>。</p>
<p>     令R是正交阵，。如果将A替换成A’。那么。s分布没变，因此x’仍然是均值为0，协方差</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>。</p>
<p>     因此，不管混合矩阵是A还是A’，x的分布情况是一样的，那么就无法确定混合矩阵，也就无法确定原信号。</p>
<ol start="3">
<li>密度函数和线性变换</li>
</ol>
<p>     在讨论ICA具体算法之前，我们先来回顾一下概率和线性代数里的知识。</p>
<p>     假设我们的随机变量s有概率密度函数（连续值是概率密度函数，离散值是概率）。为了简单，我们再假设s是实数，还有一个随机变量x&#x3D;As，A和x都是实数。令是x的概率密度，那么怎么求？</p>
<p>     令，首先将式子变换成，然后得到，求解完毕。可惜这种方法是错误的。比如s符合均匀分布的话（），那么s的概率密度是</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>，现在令A&#x3D;2，即x&#x3D;2s，也就是说x在[0,2]上均匀分布，可知。然而，前面的推导会得到</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>。正确的公式应该是</p>
<p>     推导方法</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     更一般地，如果s是向量，A可逆的方阵，那么上式子仍然成立。</p>
<ol start="4">
<li>ICA算法</li>
</ol>
<p>     ICA算法归功于Bell和Sejnowski，这里使用最大似然估计来解释算法，原始的论文中使用的是一个复杂的方法Infomax principal。</p>
<p>     我们假定每个有概率密度，那么给定时刻原信号的联合分布就是</p>
<p>     这个公式代表一个假设前提：每个人发出的声音信号各自独立。有了p(s)，我们可以求得p(x)</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     左边是每个采样信号x（n维向量）的概率，右边是每个原信号概率的乘积的|W|倍。</p>
<p>     前面提到过，如果没有先验知识，我们无法求得W和s。因此我们需要知道，我们打算选取一个概率密度函数赋给s，但是我们不能选取高斯分布的密度函数。在概率论里我们知道密度函数p(x)由累计分布函数（cdf）F(x)求导得到。F(x)要满足两个性质是：单调递增和在[0,1]。我们发现sigmoid函数很适合，定义域负无穷到正无穷，值域0到1，缓慢递增。我们假定s的累积分布函数符合sigmoid函数</p>
<p>     求导后</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     这就是s的密度函数。这里s是实数。</p>
<p>     如果我们预先知道s的分布函数，那就不用假设了，但是在缺失的情况下，sigmoid函数能够在大多数问题上取得不错的效果。由于上式中是个对称函数，因此E[s]&#x3D;0（s的均值为0），那么E[x]&#x3D;E[As]&#x3D;0，x的均值也是0。</p>
<p>     知道了，就剩下W了。给定采样后的训练样本</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>，样本对数似然估计如下：</p>
<p>     使用前面得到的x的概率密度函数，得</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     大括号里面是。</p>
<p>     接下来就是对W求导了，这里牵涉一个问题是对行列式|W|进行求导的方法，属于矩阵微积分。这里先给出结果，在文章最后再给出推导公式。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     最终得到的求导后公式如下，的导数为（可以自己验证）：</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     其中是梯度上升速率，人为指定。</p>
<p>     当迭代求出W后，便可得到来还原出原始信号。</p>
<p>注意：我们计算最大似然估计时，假设了与之间是独立的，然而对于语音信号或者其他具有时间连续依赖特性（比如温度）上，这个假设不能成立。但是在数据足够多时，假设独立对效果影响不大，同时如果事先打乱样例，并运行随机梯度上升算法，那么能够加快收敛速度。</p>
<p>     回顾一下鸡尾酒宴会问题，s是人发出的信号，是连续值，不同时间点的s不同，每个人发出的信号之间独立（和之间独立）。s的累计概率分布函数是sigmoid函数，但是所有人发出声音信号都符合这个分布。A（W的逆阵）代表了s相对于x的位置变化，x是s和A变化后的结果。</p>
<ol start="5">
<li>实例</li>
</ol>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     s&#x3D;2时的原始信号</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     观察到的x信号</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     使用ICA还原后的s信号</p>
<ol start="6">
<li>行列式的梯度</li>
</ol>
<p>     对行列式求导，设矩阵A是n×n的，我们知道行列式与代数余子式有关，</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>是去掉第i行第j列后的余子式，那么对求导得</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>     adj(A)跟我们线性代数中学的是一个意思，因此</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>YaK ： yet another knowledge , 彼岸的知识。</p>
<p>我们提供一把钥匙给孩子，去探索数字世界–学习编程和软件知识，并打开未来世界窗口。</p>
<p>让更多的孩子享受公平教育；让孩子拥有更强的能力去面对未来不确定的社会；让孩子能沉浸在快乐的学习过程中。</p>
<p><img src="https://mp.weixin.qq.com/data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="></p>
<p>需要自我的存在，发现思想的力量。从元状态中寻找科学之美，理解自我的魅力。突破思维的局限。在多学科交叉的状态下，寻找更多存在的可能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%B8%BB%E6%88%90%E4%BB%BD%E5%88%86%E6%9E%90(PCA)%E6%9C%80%E8%AF%A6%E7%BB%86%E5%92%8C%E5%85%A8%E9%9D%A2%E7%9A%84%E8%AF%A0%E9%87%8A(2)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/%E4%B8%BB%E6%88%90%E4%BB%BD%E5%88%86%E6%9E%90(PCA)%E6%9C%80%E8%AF%A6%E7%BB%86%E5%92%8C%E5%85%A8%E9%9D%A2%E7%9A%84%E8%AF%A0%E9%87%8A(2)/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:24" itemprop="dateModified" datetime="2022-11-07T06:06:24+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>主成分分析（Principal components analysis）</p>
<p>-最大方差解释</p>
<ol>
<li>问题</li>
</ol>
<p>     真实的训练数据总是存在各种各样的问题：</p>
<p>1、 比如拿到一个汽车的样本，里面既有以“千米&#x2F;每小时”度量的最大速度特征，也有“英里&#x2F;小时”的最大速度特征，显然这两个特征有一个多余。</p>
<p>2、 拿到一个数学系的本科生期末考试成绩单，里面有三列，一列是对数学的兴趣程度，一列是复习时间，还有一列是考试成绩。我们知道要学好数学，需要有浓厚的兴趣，所以第二项与第一项强相关，第三项和第二项也是强相关。那是不是可以合并第一项和第二项呢？</p>
<p>3、 拿到一个样本，特征非常多，而样例特别少，这样用回归去直接拟合非常困难，容易过度拟合。比如北京的房价：假设房子的特征是（大小、位置、朝向、是否学区房、建造年代、是否二手、层数、所在层数），搞了这么多特征，结果只有不到十个房子的样例。要拟合房子特征-&gt;房价的这么多特征，就会造成过度拟合。</p>
<p>4、 这个与第二个有点类似，假设在IR中我们建立的文档-词项矩阵中，有两个词项为“learn”和“study”，在传统的向量空间模型中，认为两者独立。然而从语义的角度来讲，两者是相似的，而且两者出现频率也类似，是不是可以合成为一个特征呢？</p>
<p>5、 在信号传输过程中，由于信道不是理想的，信道另一端收到的信号会有噪音扰动，那么怎么滤去这些噪音呢？</p>
<p>     回顾我们之前介绍的《模型选择和规则化》，里面谈到的特征选择的问题。但在那篇中要剔除的特征主要是和类标签无关的特征。比如“学生的名字”就和他的“成绩”无关，使用的是互信息的方法。</p>
<p>     而这里的特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。</p>
<p>     下面探讨一种称作主成分分析（PCA）的方法来解决部分上述问题。PCA的思想是将n维特征映射到k维上（k</p>
<ol start="2">
<li>PCA计算过程</li>
</ol>
<p>     首先介绍PCA的计算过程， 假设我们得到的2维数据如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F57DCF020054444C586BD23A482609F8A.webp"></p>
<p>     行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。也可以认为有10辆汽车，x是千米&#x2F;小时的速度，y是英里&#x2F;小时的速度，等等。</p>
<p>第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9FD9D42EA2C6444F93545EA1955D0C6D.webp"></p>
<p>第二步，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD40BA730DE3149CF9AB1D4111A444A07.webp"></p>
<p>     这里只有x和y，求解得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD4F7C06D34DF4E28A17DEEB3CC7E7CBD.webp"></p>
<p>     对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。</p>
<p>第三步，求协方差的特征值和特征向量，得到</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F34470A3CA1584C5896D27772FC0367B3.webp"></p>
<p>     上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F3160A8CD784A4C7999893A4C8435B079.webp"></p>
<p>，这里的特征向量都归一化为单位向量。</p>
<p>第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。</p>
<p>     这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FE195C50C70E44A249271650FFEEAC227.webp"></p>
<p>。</p>
<p>第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m<em>n)，协方差矩阵是n</em>n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA2563AFEC9B94C76829469E04198DFD8.webp"></p>
<p>     这里是</p>
<p>     FinalData(10<em>1) &#x3D; DataAdjust(10</em>2矩阵)×特征向量</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FE195C50C70E44A249271650FFEEAC227.webp"></p>
<p>     得到结果是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F30FC7A08E6434F6CAFAF2E079A08A823.webp"></p>
<p>     这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。</p>
<p>     上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。</p>
<p>     上述过程有个图描述：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F857BA351B69E4F2A88D1C45AE4DDEFE1.webp"></p>
<p>     正号表示预处理后的样本点，斜着的两条线就分别是正交的特征向量（由于协方差矩阵是对称的，因此其特征向量正交），最后一步的矩阵乘法就是将原始样本点分别往特征向量对应的轴上做投影。</p>
<p>     如果取的k&#x3D;2，那么结果是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F02809DFBD95E450FB3553FD85CE8D794.webp"></p>
<p>     这就是经过PCA处理后的样本数据，水平轴（上面举例为LS特征）基本上可以代表全部样本点。整个过程看起来就像将坐标系做了旋转，当然二维可以图形化表示，高维就不行了。上面的如果k&#x3D;1，那么只会留下这里的水平轴，轴上是所有点在该轴的投影。</p>
<p>     这样PCA的过程基本结束。在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差，然后对每个样例在该特征下的数据除以。</p>
<p>     归纳一下，使用我们之前熟悉的表示方法，在求协方差之前的步骤是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB24362B6D91A46A88859A42113888212.webp"></p>
<p>     其中是样例，共m个，每个样例n个特征，也就是说是n维向量。是第i个样例的第j个特征。是样例均值。是第j个特征的标准差。</p>
<p>     整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的k维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？</p>
<ol start="3">
<li>PCA理论基础</li>
</ol>
<p>     要解释为什么协方差矩阵的特征向量就是k维理想特征，我看到的有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。</p>
<p>3.1 最大方差理论</p>
<p>     在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在横轴上的投影方差较大，在纵轴上的投影方差较小，那么认为纵轴上的投影是由噪声引起的。</p>
<p>因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。</p>
<p>     比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF37AEA37E4BE43329C231E4EB2D30A9D.webp"></p>
<p>     下面将样本投影到某一维上，这里用一条过原点的直线表示（前处理的过程实质是将原点移到样本点的中心点）。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC2453652DA114675B247D2AAC872DD11.webp"></p>
<p>     假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。</p>
<p>     这里先解释一下投影的概念：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA2DB4E6C8805450B9EC7BD994626EC1B.webp"></p>
<p>     红色点表示样例，蓝色点表示在u上的投影，u是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是在u上的投影点，离原点的距离是（即或者）由于这些样本点（样例）的每一维特征均值都为0，因此投影到u上的样本点（只有一个到原点的距离值）的均值仍然是0。</p>
<p>     回到上面左右图中的左图，我们要求的是最佳的u，使得投影后的样本点方差最大。</p>
<p>     由于投影后均值为0，因此方差为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB15B9EBC2C074E8696BEF98678210768.webp"></p>
<p>     中间那部分很熟悉啊，不就是样本特征的协方差矩阵么（的均值为0，一般协方差矩阵都除以m-1，这里用m）。</p>
<p>     用来表示，表示，那么上式写作</p>
<p>     由于u是单位向量，即，上式两边都左乘u得，</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5E91333F9AD44E72897EEC5351F3220D.webp"></p>
<p>     即</p>
<p>     We got it！就是的特征值，u是特征向量。最佳的投影直线是特征值最大时对应的特征向量，其次是第二大对应的特征向量，依次类推。</p>
<p>     因此，我们只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，样例通过以下变换可以得到新的样本。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F42C891E2551D4AD498CBEE10BCCBEFAF.webp"></p>
<p>     其中的第j维就是在上的投影。</p>
<p>     通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。</p>
<p>主成分分析（Principal components analysis）-最小平方误差解释</p>
<p>3.2 最小平方误差理论</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1542617B22C844B4AE05D84EA3A610F6.webp"></p>
<p>     假设有这样的二维样本点（红色点），回顾我们前面探讨的是求一条直线，使得样本点投影到直线上的点的方差最大。本质是求直线，那么度量直线求的好不好，不仅仅只有方差最大化的方法。再回想我们最开始学习的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。比如这个问题中，特征是x，类标签是y。回归时最小二乘法度量的是距离d。如果使用回归方法来度量最佳直线，那么就是直接在原始样本上做回归了，跟特征选择就没什么关系了。</p>
<p>     因此，我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离d’来度量。</p>
<p>     现在有n个样本点，每个样本点为m维（这节内容中使用的符号与上面的不太一致，需要重新理解符号的意义）。将样本点在直线上的投影记为，那么我们就是要最小化</p>
<p>     这个公式称作最小平方误差（Least Squared Error）。</p>
<p>     而确定一条直线，一般只需要确定一个点，并且确定方向即可。</p>
<p>第一步确定点：</p>
<p>     假设要在空间中找一点来代表这n个样本点，“代表”这个词不是量化的，因此要量化的话，我们就是要找一个m维的点，使得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F063BD21257284EF795F4DA9D68898AA4.webp"></p>
<p>     最小。其中是平方错误评价函数（squared-error criterion function），假设m为n个样本点的均值：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7ABA75A4C2F8402E9C8ED19045C20DC0.webp"></p>
<p>     那么平方错误可以写作：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FE59C7AA7567240FC930E88EAE8872C82.webp"></p>
<p>     后项与无关，看做常量，而，因此最小化时，</p>
<p>是样本点均值。</p>
<p>第二步确定方向：</p>
<p>     我们从拉出要求的直线（这条直线要过点m），假设直线的方向是单位向量e。那么直线上任意一点，比如就可以用点m和e来表示</p>
<p>     其中是到点m的距离。</p>
<p>     我们重新定义最小平方误差：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F896CDF85770449758030453A4C9B0707.webp"></p>
<p>     这里的k只是相当于i。就是最小平方误差函数，其中的未知参数是和e。</p>
<p>     实际上是求的最小值。首先将上式展开：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F394D35865CF647D79EEC83AB288879FF.webp"></p>
<p>     我们首先固定e，将其看做是常量，，然后对进行求导，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5DF2FBC5F7C14470AB4339528568D54F.webp"></p>
<p>     这个结果意思是说，如果知道了e，那么将与e做内积，就可以知道了在e上的投影离m的长度距离，不过这个结果不用求都知道。</p>
<p>     然后是固定，对e求偏导数，我们先将公式（8）代入，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F941355E570B245549F79E15D518D62EC.webp"></p>
<p>     其中</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB64914344CC6437F85F6444510552A99.webp"></p>
<p> 与协方差矩阵类似，只是缺少个分母n-1，我们称之为散列矩阵（scatter matrix）。</p>
<p>     然后可以对e求偏导数，但是e需要首先满足，引入拉格朗日乘子，来使最大（最小），令</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F88C20904269A4F1D8F27A38238F3850B.webp"></p>
<p>     求偏导</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F620973D8413143B39411D6816002408B.webp"></p>
<p>     这里存在对向量求导数的技巧，方法这里不多做介绍。可以去看一些关于矩阵微积分的资料，这里求导时可以将看作是，将看做是。</p>
<p>     导数等于0时，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FAC50183E0C504065BDEE3F1D81CE2921.webp"></p>
<p>     两边除以n-1就变成了，对协方差矩阵求特征值向量了。</p>
<p>     从不同的思路出发，最后得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FCDE8B0ABFEC54E65811EE4E49DBA5AC7.webp"></p>
<p>     这时候点都聚集在新的坐标轴周围，因为我们使用的最小平方误差的意义就在此。</p>
<ol start="4">
<li>PCA理论意义</li>
</ol>
<p>     PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。再看回顾一下PCA的效果。经过PCA处理后，二维数据投影到一维上可以有以下几种情况：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F6155B8EDB2DF4C4F90183F25A60C0AAF.webp"></p>
<p>     我们认为左图好，一方面是投影后方差最大，一方面是点到直线的距离平方和最小，而且直线过样本点的中心点。为什么右边的投影效果比较差？直觉是因为坐标轴之间相关，以至于去掉一个坐标轴，就会使得坐标点无法被单独一个坐标轴确定。</p>
<p>     PCA得到的k个坐标轴实际上是k个特征向量，由于协方差矩阵对称，因此k个特征向量正交。看下面的计算过程。</p>
<p>     假设我们还是用</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F53D29B86480D417DAEA9141AD004DDB5.webp"></p>
<p>来表示样例，m个样例，n个特征。特征向量为e，表示第i个特征向量的第1维。那么原始样本特征方程可以用下面式子来表示：</p>
<p>     前面两个矩阵乘积就是协方差矩阵（除以m后），原始的样本矩阵A是第二个矩阵m*n。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC28EE1F4F6E2430CAD4BAF15BE1103CE.webp"></p>
<p>     上式可以简写为</p>
<p>     我们最后得到的投影结果是，E是k个特征向量组成的矩阵，展开如下：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7235F3537093448EA6360498D43A3560.webp"></p>
<p>     得到的新的样例矩阵就是m个样例到k个特征向量的投影，也是这k个特征向量的线性组合。e之间是正交的。从矩阵乘法中可以看出，PCA所做的变换是将原始样本点（n维），投影到k个正交的坐标系中去，丢弃其他维度的信息。举个例子，假设宇宙是n维的（霍金说是11维的），我们得到银河系中每个星星的坐标（相对于银河系中心的n维向量），然而我们想用二维坐标去逼近这些样本点，假设算出来的协方差矩阵的特征向量分别是图中的水平和竖直方向，那么我们建议以银河系中心为原点的x和y坐标轴，所有的星星都投影到x和y上，得到下面的图片。然而我们丢弃了每个星星离我们的远近距离等信息。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDFE93149C411420F9217794061B99D2A.webp"></p>
<ol start="5">
<li>总结与讨论</li>
</ol>
<p>     这一部分来自<a target="_blank" rel="noopener" href="http://www.cad.zju.edu.cn/home/chenlu/pca.htm">http://www.cad.zju.edu.cn/home/chenlu/pca.htm</a></p>
<p>     PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p>
<p>     PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 </p>
<p>但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1C9BE07DDB164D528327CCA312C8E237.gif"></p>
<p>图表 4：黑色点表示采样数据，排列成转盘的形状。 </p>
<p>     容易想象，该数据的主元是或是旋转角。</p>
<p>     如图表 4中的例子，PCA找出的主元将是。但是这显然不是最优和最简化的主元。之间存在着非线性的关系。根据先验的知识可知旋转角是最优的主元（类比极坐标）。则在这种情况下，PCA就会失效。但是，如果加入先验的知识，对数据进行某种划归，就可以将数据转化为以为线性的空间中。这类根据先验知识对数据预先进行非线性转换的方法就成为kernel-PCA，它扩展了PCA能够处理的问题的范围，又可以结合一些先验约束，是比较流行的方法。</p>
<p>     有时数据的分布并不是满足高斯分布。如图表 5所示，在非高斯分布的情况下，PCA方法得出的主元可能并不是最优的。在寻找主元时不能将方差作为衡量重要性的标准。要根据数据的分布情况选择合适的描述完全分布的变量，然后根据概率分布式</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F49DDC8041BD84412A83824C760C05B9E.gif"></p>
<p>     来计算两个向量上数据分布的相关性。等价的，保持主元间的正交假设，寻找的主元同样要使。这一类方法被称为独立主元分解(ICA)。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA7D425DDC60A4D96922297CD7DBAFF20.gif"></p>
<p>图表 5：数据的分布并不满足高斯分布，呈明显的十字星状。 </p>
<p>     这种情况下，方差最大的方向并不是最优主元方向。</p>
<p>     另外PCA还可以用于预测矩阵中缺失的元素。</p>
<p>独立成分分析（Independent Component Analysis）</p>
<ol>
<li>问题：</li>
</ol>
<p>     1、上节提到的PCA是一种数据降维的方法，但是只对符合高斯分布的样本点比较有效，那么对于其他分布的样本，有没有主元分解的方法呢？</p>
<p>     2、经典的鸡尾酒宴会问题（cocktail party problem）。假设在party中有n个人，他们可以同时说话，我们也在房间中一些角落里共放置了n个声音接收器（Microphone）用来记录声音。宴会过后，我们从n个麦克风中得到了一组数据</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F81CFDFE900664CF1B15746F4EDD7AB72.webp"></p>
<p>，i表示采样的时间顺序，也就是说共得到了m组采样，每一组采样都是n维的。我们的目标是单单从这m组采样数据中分辨出每个人说话的信号。</p>
<p>     将第二个问题细化一下，有n个信号源，，每一维都是一个人的声音信号，每个人发出的声音信号独立。A是一个未知的混合矩阵（mixing matrix），用来组合叠加信号s，那么</p>
<p>     x的意义在上文解释过，这里的x不是一个向量，是一个矩阵。其中每个列向量是，</p>
<p>     表示成图就是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA46BCB0604284CF1960DB8A09CF105C2.webp"></p>
<p>     这张图来自</p>
<p><a target="_blank" rel="noopener" href="http://amouraux.webnode.com/research-interests/research-interests-erp-analysis/blind-source-separation-bss-of-erps-using-independent-component-analysis-ica/">http://amouraux.webnode.com/research-interests/research-interests-erp-analysis/blind-source-separation-bss-of-erps-using-independent-component-analysis-ica/</a></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F340D7B2DC5BE411F844817147139813C.webp"></p>
<p>的每个分量都由的分量线性表示。A和s都是未知的，x是已知的，我们要想办法根据x来推出s。这个过程也称作为盲信号分离。</p>
<p>     令，那么</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F738836508E384C16B9ED0571DE76A036.webp"></p>
<p>     将W表示成</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F35FE65E546D844E9916EE2122EBC9BA0.webp"></p>
<p>     其中，其实就是将写成行向量形式。那么得到：</p>
<ol start="2">
<li>ICA的不确定性（ICA ambiguities）</li>
</ol>
<p>     由于w和s都不确定，那么在没有先验知识的情况下，无法同时确定这两个相关参数。比如上面的公式s&#x3D;wx。当w扩大两倍时，s只需要同时扩大两倍即可，等式仍然满足，因此无法得到唯一的s。同时如果将人的编号打乱，变成另外一个顺序，如上图的蓝色节点的编号变为3,2,1，那么只需要调换A的列向量顺序即可，因此也无法单独确定s。这两种情况称为原信号不确定。</p>
<p>     还有一种ICA不适用的情况，那就是信号不能是高斯分布的。假设只有两个人发出的声音信号符合多值正态分布，，I是2*2的单位矩阵，s的概率密度函数就不用说了吧，以均值0为中心，投影面是椭圆的山峰状（参见多值高斯分布）。因为，因此，x也是高斯分布的，均值为0，协方差为</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1002F08D53E34D69B08A748F3E58E557.webp"></p>
<p>。</p>
<p>     令R是正交阵，。如果将A替换成A’。那么。s分布没变，因此x’仍然是均值为0，协方差</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9CDFB321ED9E4B9689B35A326BA1292A.webp"></p>
<p>。</p>
<p>     因此，不管混合矩阵是A还是A’，x的分布情况是一样的，那么就无法确定混合矩阵，也就无法确定原信号。</p>
<ol start="3">
<li>密度函数和线性变换</li>
</ol>
<p>     在讨论ICA具体算法之前，我们先来回顾一下概率和线性代数里的知识。</p>
<p>     假设我们的随机变量s有概率密度函数（连续值是概率密度函数，离散值是概率）。为了简单，我们再假设s是实数，还有一个随机变量x&#x3D;As，A和x都是实数。令是x的概率密度，那么怎么求？</p>
<p>     令，首先将式子变换成，然后得到，求解完毕。可惜这种方法是错误的。比如s符合均匀分布的话（），那么s的概率密度是</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FC6F41964D0224B7F97092E17E15C20D0.webp"></p>
<p>，现在令A&#x3D;2，即x&#x3D;2s，也就是说x在[0,2]上均匀分布，可知。然而，前面的推导会得到</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F3F5151121F63497BB8F38B8ADE5292E1.webp"></p>
<p>。正确的公式应该是</p>
<p>     推导方法</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F309AD0C0FE8B4DF89C820517AA3AF139.webp"></p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F3211AE854C6D441AB956D7AEEAE35261.webp"></p>
<p>     更一般地，如果s是向量，A可逆的方阵，那么上式子仍然成立。</p>
<ol start="4">
<li>ICA算法</li>
</ol>
<p>     ICA算法归功于Bell和Sejnowski，这里使用最大似然估计来解释算法，原始的论文中使用的是一个复杂的方法Infomax principal。</p>
<p>     我们假定每个有概率密度，那么给定时刻原信号的联合分布就是</p>
<p>     这个公式代表一个假设前提：每个人发出的声音信号各自独立。有了p(s)，我们可以求得p(x)</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2030DCA1C55D4C2E87BC1E30EF32AE54.webp"></p>
<p>     左边是每个采样信号x（n维向量）的概率，右边是每个原信号概率的乘积的|W|倍。</p>
<p>     前面提到过，如果没有先验知识，我们无法求得W和s。因此我们需要知道，我们打算选取一个概率密度函数赋给s，但是我们不能选取高斯分布的密度函数。在概率论里我们知道密度函数p(x)由累计分布函数（cdf）F(x)求导得到。F(x)要满足两个性质是：单调递增和在[0,1]。我们发现sigmoid函数很适合，定义域负无穷到正无穷，值域0到1，缓慢递增。我们假定s的累积分布函数符合sigmoid函数</p>
<p>     求导后</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F972AAC41CA2D4FABBD7E98EEB5388E82.png"></p>
<p>     这就是s的密度函数。这里s是实数。</p>
<p>     如果我们预先知道s的分布函数，那就不用假设了，但是在缺失的情况下，sigmoid函数能够在大多数问题上取得不错的效果。由于上式中是个对称函数，因此E[s]&#x3D;0（s的均值为0），那么E[x]&#x3D;E[As]&#x3D;0，x的均值也是0。</p>
<p>     知道了，就剩下W了。给定采样后的训练样本</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1DBBD4F571174BE4BE43E501CDE5258C.webp"></p>
<p>，样本对数似然估计如下：</p>
<p>     使用前面得到的x的概率密度函数，得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FEC4E29E060B247068D3F53B2AA08C8F6.webp"></p>
<p>     大括号里面是。</p>
<p>     接下来就是对W求导了，这里牵涉一个问题是对行列式|W|进行求导的方法，属于矩阵微积分。这里先给出结果，在文章最后再给出推导公式。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD9EFBE5010494C808C33E9B495F21095.webp"></p>
<p>     最终得到的求导后公式如下，的导数为（可以自己验证）：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F546590D5ECC9493C9DF6D7BB309768B7.webp"></p>
<p>     其中是梯度上升速率，人为指定。</p>
<p>     当迭代求出W后，便可得到来还原出原始信号。</p>
<p>注意：我们计算最大似然估计时，假设了与之间是独立的，然而对于语音信号或者其他具有时间连续依赖特性（比如温度）上，这个假设不能成立。但是在数据足够多时，假设独立对效果影响不大，同时如果事先打乱样例，并运行随机梯度上升算法，那么能够加快收敛速度。</p>
<p>     回顾一下鸡尾酒宴会问题，s是人发出的信号，是连续值，不同时间点的s不同，每个人发出的信号之间独立（和之间独立）。s的累计概率分布函数是sigmoid函数，但是所有人发出声音信号都符合这个分布。A（W的逆阵）代表了s相对于x的位置变化，x是s和A变化后的结果。</p>
<ol start="5">
<li>实例</li>
</ol>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB529A4619B8A4CA4A8EA08C7722D9E5F.webp"></p>
<p>     s&#x3D;2时的原始信号</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FECB5A01BB7AF4EC894518CA8D4DDD304.webp"></p>
<p>     观察到的x信号</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F4B0368ECBCE6433EAB1917F3EFA69AEE.webp"></p>
<p>     使用ICA还原后的s信号</p>
<ol start="6">
<li>行列式的梯度</li>
</ol>
<p>     对行列式求导，设矩阵A是n×n的，我们知道行列式与代数余子式有关，</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F7025A49BD76F471CB0E1F56AECF493CF.webp"></p>
<p>是去掉第i行第j列后的余子式，那么对求导得</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F698B0957229E46EE9116AE74A8199BA3.webp"></p>
<p>     adj(A)跟我们线性代数中学的是一个意思，因此</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F2FB4E6CCFFD04CA2A051FE5B57446B4D.webp"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/07/005_%E6%95%B0%E5%AD%A6/PCA%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Quentin">
      <meta itemprop="description" content="选择有时候比努力更重要">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Quentin 的世界">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2023/03/07/005_%E6%95%B0%E5%AD%A6/PCA%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-03-07 21:14:26" itemprop="dateCreated datePublished" datetime="2023-03-07T21:14:26+08:00">2023-03-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-11-07 06:06:58" itemprop="dateModified" datetime="2022-11-07T06:06:58+08:00">2022-11-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。</p>
<h2 id="1、PCA的数学原理"><a href="#1、PCA的数学原理" class="headerlink" title="1、PCA的数学原理"></a>1、PCA的数学原理</h2><p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，可以在降维的同时将信息的损失尽量降低。</p>
<h3 id="基变换的矩阵表示"><a href="#基变换的矩阵表示" class="headerlink" title="基变换的矩阵表示"></a>基变换的矩阵表示</h3><p>如果有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵P，然后将向量按列组成矩阵A，那么两矩阵的乘积PA就是变换结果，N维向量组成的矩阵A变为R维向量组成的矩阵PA，数学表示为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FBE667075DFA94FACADAF0D2BD888ED9D.webp"></p>
<p>R小于N，及实现了降维。将N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。矩阵相乘可以表示降维变换。矩阵相乘的物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。</p>
<h3 id="协方差矩阵及优化目标"><a href="#协方差矩阵及优化目标" class="headerlink" title="协方差矩阵及优化目标"></a>协方差矩阵及优化目标</h3><p>如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到R维（R小于N），那么我们应该如何选择R个基才能最大程度保留原有的信息？寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<p>对于二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。数学上可以用两个字段的协方差表示其相关性，由于已经让每个行均值为0，则：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDEB7E2BDA8BC4912933176BF00D7226F.webp"></p>
<p>可以看到，在行均值为0的情况下，两行的协方差简洁的表示为其内积除以元素数m。当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>
<p>至此，我们得到了降维问题的优化目标：将一组N维向量降为R维（0&lt;R&lt;N），其目标是选择R个单位（模为1）正交基，使得原始数据变换到这组基上后，各行向量两两间协方差为0，而行向量的方差则尽可能大（在正交的约束下，取最大的R个方差）。</p>
<h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>有a和b两个字段，将它们按行组成矩阵X：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FE40EF210D6794697AAA24A5F83AD8E67.webp"></p>
<p>然后用X乘以X的转置，并乘上系数1&#x2F;m：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FF73A23744D154EBE83A1CA0AF74A947C.webp"></p>
<p>得到协方差矩阵，矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。</p>
<h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><p>要达到降维问题的优化目标，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列。</p>
<p>原始数据矩阵X，X的协方差矩阵C，而P是一组基按行组成的对角矩阵，Y的协方差矩阵为D，Y为X对P做基变换后的数据。则 Y&#x3D;PX，下面：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA46ABE0BB1224C2A9DFAB3DE50C57B0D.webp"></p>
<p>矩阵P，满足 是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前R行就是要寻找的基，用P的前R行组成的矩阵乘以X就使得X从N维降到了R维并满足上述优化条件。</p>
<p>由上文知道，X的协方差矩阵C是一个实对称矩阵，实对称矩阵有一系列非常好的性质：</p>
<p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p>
<p>2）设特征向量λ重数为r，则必然存在r个线性无关的特征向量对应于λλ，因此可以将这r个特征向量单位正交化。</p>
<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为，我们将其按列组成矩阵：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F21D9D83BD0C442FC9104131C1C54D9BE.webp"></p>
<p>Λ为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FB23230780EE7456C8613F2128925F8A5.webp"></p>
<p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照Λ中特征值的从大到小，将特征向量从上到下排列，则用P的前R行组成的矩阵乘以原始数据矩阵X，就得到了降维后的数据矩阵Y。</p>
<h2 id="2、算法及实例"><a href="#2、算法及实例" class="headerlink" title="2、算法及实例"></a>2、算法及实例</h2><p>PCA算法</p>
<p>总结一下PCA的算法步骤：</p>
<p>设有m条n维数据。</p>
<p>1）将原始数据按列组成n行m列矩阵X</p>
<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
<p>3）求出协方差矩阵</p>
<p>4）求出协方差矩阵的特征值及对应的特征向量</p>
<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前r行组成矩阵P</p>
<p>6）Y&#x3D;PX即为降维到r维后的数据</p>
<p>实例</p>
<p>用PCA方法将这组二维数据其降到一维。</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F5B9EB43383614E7987EB5F31437BFFC5.webp"></p>
<p>因为这个矩阵的每行已经是零均值，这里直接求协方差矩阵：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FDDAE4B451BC94163AF35FAC272DBF378.webp"></p>
<p>求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F899331B23C654C5BA06A8D2EAD632B8B.webp"></p>
<p>其对应的特征向量分别是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F9FD577EBAA6648C3A8AFADBEA7DBC5FE.webp"></p>
<p>其中对应的特征向量分别是一个通解，c1和c2可取任意实数。那么标准化后的特征向量为：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA7B402AAE487484780E2EE1B31063679.webp"></p>
<p>因此矩阵P是：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FD252A128C0504556B54D8AFD5D2AA2AF.webp"></p>
<p>可以验证协方差矩阵C的对角化：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F1C1AFDF7FF924346AA7A33F403BDBF06.webp"></p>
<p>最后用P的第一行乘以数据矩阵，就得到了降维后的表示：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2F53B2A91C137A4637A86E2BA02D0DF524.webp"></p>
<p>降维投影结果如下图：</p>
<p><img src="https://quentin-1305162271.cos.eu-frankfurt.myqcloud.com/youdaonote-images%2FA78759C68D76424CA7C985F27F6FC446.webp"></p>
<p>进一步讨论</p>
<p>PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>
<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>
<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。PCA的数学原理</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Quentin</p>
  <div class="site-description" itemprop="description">选择有时候比努力更重要</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Quentin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
